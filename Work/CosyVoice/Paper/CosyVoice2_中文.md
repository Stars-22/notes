# CosyVoice 2:使用大语言模型的可扩展流式语音合成

**作者**: Zhihao Du, Yuxuan Wang, Qian Chen, Xian Shi, Xiang Lv, Tianyu Zhao, Zhifu Gao, Yexin Yang, Changfeng Gao, Hui Wang, Fan Yu, Huadai Liu, Zhengyan Sheng, Yue Gu, Chong Deng, Wen Wang, Shiliang Zhang, Zhijie Yan, Jingren Zhou*

**机构**: 阿里巴巴集团, 中国

**邮箱**: {neo.dzh,sly.zsl}@alibaba-inc.com

---

## 摘要

在之前的工作中,我们介绍了CosyVoice,一个基于监督离散语音标记的多语言语音合成模型。通过采用两种流行生成模型(语言模型和流匹配)的渐进式语义解码,CosyVoice在语音上下文学习中展示了高韵律自然度、内容一致性和说话人相似度。

最近,多模态大语言模型(LLM)取得了显著进展,其中语音合成的响应延迟和实时因子在交互体验中起着关键作用。因此,在本报告中,我们提出了一个改进的流式语音合成模型CosyVoice 2,它结合了全面和系统的优化。

具体而言,我们引入有限标量量化来改善语音标记的码本利用率。对于文本-语音LM,我们简化了模型架构,允许直接使用预训练LLM作为骨干。

此外,我们开发了一个分块感知因果流匹配模型来支持各种合成场景,在单个模型中实现流式和非流式合成。通过在大规模多语言数据集上训练,CosyVoice 2实现了人类同等自然度、最小响应延迟,以及在流式模式下几乎无损的合成质量。

我们邀请读者在https://funaudiollm.github.io/cosyvoice2收听演示。

---

## 1 引言

近年来,神经文本到语音(TTS)合成模型引起了广泛关注,超越了传统拼接和统计参数方法[1-7]。这些模型在预定义的特定说话人上实现了高保真度和自然度。

最近的研究表明,零样本TTS模型可以通过模仿参考语音的音色、韵律和风格为任何说话人合成语音[8]。除了其上下文学习(ICL)能力外,零样本TTS模型还从大规模训练数据中受益,实现了几乎与人类语音无法区分的合成质量和自然度。

最近的零样本TTS模型大致可分为三类:编解码器语言模型、特征扩散模型及其混合系统。

### 编解码器语言模型

利用语音编解码器模型提取离散语音表示[9-11]并采用自回归[8,12-17]或掩码[18]语言模型来预测语音标记,然后通过编解码器声码器[19,20]将其合成为波形。连续语音表示也在[21]中被探索。基于语言模型的TTS可以通过自回归采样生成多样化且韵律一致的语音。

### 扩散模型

受图像生成进展的启发,去噪扩散[22,23]和流匹配模型[24]已被引入非自回归(NAR)语音合成。

早期基于扩散的TTS模型需要为每个文本(音素)预测持续时间以解决文本和语音特征之间的长度不匹配[25-28]。然而,这种刚性对齐可能影响自然度,导致平缓的韵律。

为了缓解这个问题,交叉注意和扩散Transformer(DiT)已被引入NAR TTS模型[29,30]。最近的研究表明,NAR TTS模型中文本-语音对齐的更简单方法,如E2 TTS[31]、F5-TTS[32]和Seed-TTS[33]。

在这些模型中,输入文本用特殊标记填充以匹配总语音长度,该长度要么由语音持续时间预测模块自动预测,要么由用户预先指定。由于NAR TTS模型不受编解码器声码器约束,它们可以实现更优的语音质量。

### 混合系统

混合系统结合了文本到编解码器语言模型和编解码器到特征扩散模型[33-35]。语言模型解决文本和语音之间的对齐以及语音持续时间预测,而编解码器到特征扩散模型基于生成的编解码器和其他条件合成语音特征(梅尔谱图)。通过利用两种生成模型的优势,混合系统实现了高多样性、韵律一致性和语音质量。

尽管最近的零样本TTS模型取得了成功,但它们通常在非流式(离线)模式下运行,这涉及完整的输入文本,要求在返回波形之前合成整个语音。这导致高延迟,对语音聊天等应用中的用户体验产生负面影响[36,37]。

为了解决这个问题,已经为基于语言模型的零样本TTS模型探索了流式合成[38-41],但基于扩散的TTS模型和混合系统缺乏成熟的流式解决方案。

基于CosyVoice[34]的成功,我们介绍了CosyVoice 2,一个具有改进的韵律自然度、内容一致性和说话人相似度的流式零样本TTS模型。我们的贡献包括:

- **在单个框架中统一流式和非流式合成**,提出统一的文本-语音语言模型和分块感知因果流匹配模型,实现相对于离线模式无损的流式合成
- **简化LM架构**通过删除文本编码器和说话人嵌入,允许预训练文本大语言模型(LLM)作为骨干,增强上下文理解
- **用有限标量量化(FSQ)替换语音标记器中的向量量化(VQ)**,提高码本利用率并捕获更多语音信息
- **升级指令TTS能力**以支持更多指令,包括情感、口音、角色风格和细粒度控制。在CosyVoice 2中,指令和零样本能力集成到单个模型中,实现更多样化和生动的合成

通过上述系统修改和优化,CosyVoice 2实现了人类同等合成质量,在流式模式下几乎无损。统一框架放宽了部署要求,使单个模型能够支持流式和非流式合成。升级的指令TTS能力为用户提供了更强大且更容易的方法来生成各种语音。此外,分块感知流匹配设计也可以应用于NAR TTS模型,这表明了流式NAR模型的潜力。

---

## 2 CosyVoice 2

CosyVoice 2建立在其前身[34]的类似设计理念上,通过分离语音信号的语义和声学信息并独立建模它们。语音生成过程被重新定义为渐进式语义解码过程,其中条件信息被逐步结合。

具体而言,文本-语音语言模型(LM)仅关注语义信息,将高级文本标记解码为监督语义语音标记。在流匹配模型中,音色等声学细节通过说话人嵌入和参考语音引入,将语音标记转换为给定说话人的梅尔谱图。最后,预训练声码器模型恢复相位,将梅尔谱图转换回原始音频信号。

以下章节将从五个方面介绍CosyVoice 2的细节和流式合成的修改:文本标记器、监督语义语音标记器、用于流式/非流式合成的统一文本-语音LM以及分块感知流匹配模型。图1提供了CosyVoice 2的概述。

### 2.1 文本标记器

CosyVoice 2直接使用原始文本作为输入,使用基于BPE的文本标记器对其进行标记化。这消除了通过图素到音素(g2p)转换获得音素的前端模型的需求。

这种方法不仅简化了数据预处理工作流,还使模型能够以端到端方式在各种上下文中学习单词的发音。与文本LLM中常用的标记器不同,CosyVoice 2屏蔽掉一对多标记。这防止标记的发音变得过长,并减少数据稀疏性引起的边缘情况。

具体而言,如果BPE标记编码超过一个中文字符,它将被屏蔽,并且在标记化过程中每个字符将被单独编码。其他语言,如英语、日语和韩语,不进行特殊处理。

### 2.2 监督语义语音标记器

如图1(a)所示,我们在SenseVoice-Large ASR模型[43]的编码器中插入有限标量量化(FSQ)模块[42]。在训练阶段,输入语音X经过Encoder1获得中间表示,其中Encoder1由六个Transformer块组成,带有旋转位置嵌入[44]。

然后,中间表示被输入到FSQ模块进行量化,量化表示经过SenseVoice-Large的剩余模块,包括Encoder2和ASR解码器,以预测相应文本标记的后验概率。

在FSQ模块中,中间表示H首先被投影到D维低秩空间,每个维度的值被量化为[-K, K]并使用有界舍入操作ROUND。然后,量化的低秩表示H̄被投影到原始维度H̃用于以下模块:

```
H̄ = ROUND(Projdown(H))  (1)
Ĥ = Projup(H̄)  (2)
```

在训练阶段,使用直通估计来近似FSQ模块和Encoder1的梯度。语音标记μi可以通过计算量化低秩表示h̄i在(2K + 1)进制系统中的索引获得:

```
μi = Σ_{j=0}^{D-1} h̄i,j (2K + 1)^j  (3)
```

Encoder1、FSQ模块的低秩投影器、有界舍入操作和索引计算构成了CosyVoice 2的语音标记器。我们的语音标记器工作在25 Hz的标记率,即每秒25个语音标记。

### 2.3 统一文本-语音语言模型

在CosyVoice 2中,预训练文本LLM Qwen2.5-0.5B[45]被用作文本-语音语言模型,以输入文本为提示自回归地生成语音标记。

与其他LM类似,文本-语音LM也在下一个标记预测方案中训练,如图1(b)所示。与之前的CosyVoice不同,我们删除了说话人嵌入以避免信息泄露。更重要的是,我们发现这样的话语级向量不仅包含说话人身份,还包含语言和副语言信息,这损害了文本-语音LM的韵律自然度和跨语言能力。此外,我们还放弃了之前CosyVoice的文本编码器,因为我们发现Qwen2.5-0.5B模型足够强大来对齐文本和语音标记,不再需要文本编码器。

受益于文本-语音LM的简单性,我们可以为流式和非流式合成构建统一模型。这里,"流式模式"意味着输入文本以连续流接收,而不是提前作为完整句子已知。在CosyVoice 2中,流式和非流式模式的区别仅是LM的序列构造方式:

- **对于非流式模式**,"序列开始"S、所有文本标记、"语音转换"标记T、所有语音标记和"序列结束"E如图2底部所示顺序连接。忽略标记意味着在最小化交叉熵目标函数时忽略它们的损失。
- **对于流式模式**,我们以预定义的N:M比例混合文本和语音标记,即每N个文本标记后跟M个语音标记,如图2顶部所示。如果下一个标记是文本标记,模型预期预测一个填充标记(而不是文本标记),这指示推理阶段应该连接下N个文本标记。一旦文本标记用完,"语音转换"标记T和剩余语音标记顺序连接,形成流式模式下的混合文本-语音标记序列。在我们的实验中,N和M分别设置为5和15。

通过同时在上述两个序列上训练文本-语音LM,我们可以在单个统一模型中执行流式和非流式语音生成。在现实场景中,如说话人微调(SFT)和上下文学习(ICL),推理序列区别如下:

- **ICL,非流式**: 在ICL中,LM需要来自参考音频的提示文本和语音标记来模仿口音、韵律、情感和风格。在非流式模式中,提示和待合成文本标记作为整个实体连接,提示语音标记被视为预生成结果并固定:"S, 提示文本, 文本, T, 提示语音"。LM的自回归生成从这样的序列开始,直到检测到"序列结束"标记E。
- **ICL,流式**: 在这种情况下,我们假设待生成文本已知,语音标记应该以流式方式生成。类似地,我们将提示和待生成文本视为整个实体。然后,我们以N:M比例将其与提示语音标记混合:"S, 混合文本语音, T, 剩余语音"。如果文本长度长于提示语音标记,LM将生成"填充标记"。在这种情况下,我们手动填充N个文本标记。如果文本标记用完,将添加"语音转换"标记T。在流式模式中,我们每M个标记返回生成结果,直到检测到E。
- **SFT,非流式**: 在SFT场景中,LM在特定说话人上微调,不再需要提示文本和语音。因此,初始序列非常简单:"S, 文本, T"。从此开始,文本-语音LM可以自回归生成语音标记直到T。
- **SFT,流式**: 在SFT的流式模式中,我们从以下序列开始语音生成:"S, 前N个文本"。然后,LM将生成M个语音标记,我们手动填充下N个文本标记。我们重复上述过程,直到所有文本标记用完,然后添加T。请注意,此模式也可以被语音到语音多模态大语言模型采用,以获得极低延迟。

### 2.4 分块感知流匹配

在CosyVoice 2中,我们采用梅尔谱图作为声学特征,帧率为50 Hz,采样率为24000。由于语音标记和梅尔特征之间的帧率不匹配,我们以2的比率上采样语音标记以匹配梅尔谱图的帧率。在上采样操作之前,我们添加一个额外的前瞻卷积层为以下因果模块提供未来信息。前瞻层通过右填充1D卷积实现,填充大小为P,核大小为P+1。

此后,几个分块感知因果Transformer块跟随以对齐语音标记的表示空间以匹配声学特征。

随后,我们的目标是将语音标记进一步解码为由说话人嵌入和参考语音指定的梅尔谱图。为了实现这一点,我们采用条件流匹配(CFM)模型,以语音标记、参考语音和说话人嵌入为条件采样梅尔谱图。

在CFM模型中,目标梅尔谱图的分布由从先验分布p₀(X)到数据分布q(X)的概率密度路径描述。为了采样效率,我们采用最优传输(OT)流来匹配向量场ωₜ,它由常微分方程(ODE)给出:

```
ωₜ(φ_{OT}ₜ(X₀, X₁)|X₁) = X₁ - X₀  (3)
φ_{OT}ₜ(X₀, X₁) = (1 - t)X₀ + tX₁  (4)
X₀ ∼ p₀(X) = N(0, I)  (5)
X₁ ∼ q(X)  (6)
```

采用因果卷积Transformer UNet以使用上采样标记μ、掩码梅尔谱图X̃₁、说话人嵌入v和时间步t作为条件学习上述ODE:

```
νₜ(φ_{OT}ₜ(X₀, X₁)|θ) = UNetθ(φ_{OT}ₜ(X₀, X₁), t; v, {μ}_{1:L}, X̃₁)  (7)
```

在训练阶段,通过随机掩码X₁中70%到100%的最终帧获得掩码梅尔谱图。至于推理,它由从参考语音提取的梅尔谱图提供。

通过最小化预测和真实ODE之间的L1损失,我们可以如下优化UNet参数θ:

```
θ = arg minθ E_{p₀(X),q(X),t}||ωₜ(φ_{OT}ₜ(X₀, X₁)) - νₜ(φ_{OT}ₜ(X₀, X₁)|θ; μ, X̃₁, v)||  (8)
```

在训练阶段,时间步遵循均匀分布U[0, 1]。然而,在推理期间,我们采用余弦调度器为初始生成阶段提供更多步骤:

```
t := 1 - cos((tπ)/2)  (9)
```

此外,我们还在条件和非条件情况下训练模型以在推理阶段启用分类器自由引导(CFG)[46-48]:

```
ν̃ₜ(φ_{OT}ₜ(X₀, X₁)|θ; Ψ) = (1 + β) · νₜ(φ_{OT}ₜ(X₀, X₁)|θ; Ψ) - β · νₜ(φ_{OT}ₜ(X₀, X₁)|θ)  (10)
```

其中Ψ表示条件{v, μ, X̃₁}。根据实验结果,CFG强度β和流估计(NFE)数分别设置为0.7和10。

当前的流匹配模型总是在离线模式下工作,即只有所有语音标记生成后,才能采样梅尔谱图,这对流式合成不友好。为了克服这个问题,我们将多步流估计视为堆叠的更深层神经网络,重复UNet十次。因此,通过使展开的神经网络因果化,我们可以在流式合成中应用它。

我们构造四个掩码以满足不同的应用情况:

- **非因果掩码**用于离线模式,可以通过关注所有条件帧实现最佳性能。非因果掩码适合对延迟不敏感的情况。
- **全因果掩码**设计用于需要极低延迟的场景,其中只能关注过去帧。
- **分块-M掩码**是延迟和性能之间的折衷,可以利用过去和M个未来帧的信息。此掩码更适合具有低延迟的初始生成块。
- **分块-2M掩码**可以通过牺牲更多延迟实现离线模式的近似性能,可用于级联生成块以获得更好性能。

对于小批量中的每个训练情况,我们根据均匀分布从上述四个掩码中随机采样一个掩码。通过这种方式,一个流匹配模型可以兼容不同场景,降低部署复杂性。分块感知训练的另一个优势是更多上下文的掩码为较少上下文的掩码充当教师,受益于隐式自蒸馏方案。

### 2.5 流式模式延迟分析

首包延迟是流式合成模型的重要指标,特别是在LLM-based语音聊天应用如GPT-4o[36]中显著影响用户体验。在TTS的背景下,待合成文本提前已知,延迟来自语音标记生成、梅尔谱图重构和波形合成方面。

因此,CosyVoice 2的首包延迟L_TTS可以如下获得:

```
L_TTS = M · d_lm + M · d_fm + M · d_voc  (11)
```

其中d_lm表示LM生成一个语音标记的计算时间,d_fm表示流匹配模型为语音标记生成梅尔谱图帧的计算时间,d_voc表示声码器合成对应一个语音标记波形的计算时间。

在LLM-based语音聊天的背景下,首包所需文本的长度也应考虑,首包延迟L_Chat变为:

```
L_Chat ≤ N · d_llm + L_TTS  (12)
```

其中d_llm表示LLM生成一个文本标记的计算时间。请注意,由于多字符标记在CosyVoice 2的文本标记器中被屏蔽,文本LLM使用的文本标记总是比CosyVoice 2编码更长的原始文本。因此,首包延迟L_Chat必须低于N · d_llm和L_TTS的总和。

### 2.6 指令生成

为了增强CosyVoice 2的可控性,我们将指令数据集集成到基础训练集中。我们收集了1500小时的指令训练数据,包括自然语言指令和细粒度指令,如表1所概述。

对于自然语言指令,我们在待合成输入文本之前添加自然语言描述和特殊结束标记"<|endofprompt|"。这些描述涵盖诸如情感、语速、角色扮演和方言等方面。

对于细粒度指令,我们在文本标记之间插入语音突发,使用如"[laughter]"和"[breath]"等标记。此外,我们对短语应用语音特征标签;例如,"<strong>XXX</strong>"表示强调某些单词,而"<laughter>XXX</laughter>"表示边笑边说。

### 2.7 多说话人微调

在特定说话人上微调预训练模型(SFT)可以进一步提高生成质量和说话人相似度。在本报告中,我们介绍了多说话人微调(mSFT),其中预训练模型在多个说话人上同时微调而不是单个说话人。这种方法确保多个说话人之间的全面韵律和发音覆盖,并减轻预训练模型的潜在灾难性遗忘。

为了避免各种说话人之间的音色混淆,我们在特定说话人的输入文本前添加说话人提示标记"Speaker A<|endofprompt>"。如果训练样本未标记到说话人,使用特殊标记"unknown<|endofprompt|"。在整个多说话人微调过程中,学习率设置为1e-5。

### 2.8 使用强化学习的微调

强化学习是大语言模型训练中常用的方法,可以使LM输出与人类偏好对齐。在CosyVoice 2中,我们采用说话人相似度(SS)和来自ASR系统的识别词错误率(WER)作为奖励函数,以提高微调阶段的说话人相似度和发音准确度。

我们使用WER和SS来区分偏好样本x_w和拒绝样本x_l,并使用直接偏好优化(DPO)[49]如下优化TTS系统:

```
L_DPO(πθ; π_ref) = -log σ(β log(πθ(μ_w|y)/π_ref(μ_w|y) - β log(πθ(μ_l|y)/π_ref(μ_l|y))  (13)
```

其中μ_w和μ_l是从偏好和拒绝样本x_w和x_l提取的语音标记。

然而,此方法耗时且计算量大,因为它应该通过TTS系统反复合成音频以获得可区分的偏好和拒绝样本。在训练期间,一个训练步需要四个前向操作。为了简化过程,我们将LM预测的标记μi ∈ {0, 1, ..., (2K + 1)^D - 1}恢复为量化低秩表示H̄,并直接使用语音标记器的ASR后端重新预测输入文本。然后,预测的对数后验可以被视为ASR奖励函数以优化文本-语音语言模型。在训练期间,ASR后端参数被冻结。

```
h̄i,j = (μi mod (2K + 1)^j) / (2K + 1)^j  (14)
Ĥ = Projup(H̄)  (15)
L_ASR = -log P(Y|Ĥ; θ_ASR)  (16)
```

其中Y是输入文本,H̄是恢复的语音低秩表示。由于采样操作μi ∼ P(μi|μ_{1:i-1}, Y; θ_LM)仍然阻止我们直接优化模型,我们使用gumbel softmax采样使其可微分化,然后通过L_ASR优化θ_LM。

---

## 3 实验设置

### 3.1 语音标记器训练数据

使用200,000小时数据集训练语音标记器,使用规范化转录作为标签。详细数据信息列于表2。训练数据来自三个不同资源:开源ASR数据集、内部工业数据集和TTS生成数据集。

虽然我们在训练语音标记器时仅使用中英文数据,如表2所示,后续实验表明语音标记器对其他语言具有零样本能力。它也可用于日语和韩语等语言的语音合成。

**表2: 语音标记器训练数据详情**

| 语言 | 时长(小时) |
|------|-----------|
| 中文 | 110,884 |
| 英文 | 99,918 |

### 3.2 CosyVoice 2训练数据

CosyVoice 2与其前一个版本[34]共享相同的训练数据。我们首先通过内部语音处理工具收集仅语音数据。随后,使用Paraformer[50]和SenseVoice[43]分别为中文和其他语言生成伪文本标签。

我们还采用内部强制对齐模型来过滤低质量数据并增强标点符号的准确性。数据详情在表3中提供。

**表3: CosyVoice 2训练数据详情**

| 语言 | 时长(小时) |
|------|-----------|
| 中文 | 130,000 |
| 英文 | 30,000 |
| 日语 | 4,600 |
| 韩语 | 2,200 |

### 3.3 评估设置

我们在两个测试集上评估CosyVoice 2。第一个是从Librispeech语料库[51]的test-clean集构建的,表示为test-clean。此测试集用于在有限英语领域评估CosyVoice 2。使用Whisper-large V3作为ASR模型评估内容一致性。对于说话人相似度(SS),我们采用ERes2Net模型[52]提取提示和生成语音的说话人嵌入,它们的原始余弦相似度被视为说话人相似度。使用NMOS评分[53]评估客观质量。

第二个评估在SEED测试集[33]上进行,该测试集广泛用于评估最近的TTS模型,涵盖各种文本域和参考语音。在此评估中,约2,000个中文和1,000个英文样本从CommonVoice数据集中选择,分别表示为test-zh和test-en。此外,还包括约400个硬测试案例以评估TTS模型在文本重复、绕口令和其他挑战性合成案例上的鲁棒性,在本报告中表示为test-hard。使用Paraformer识别test-zh和test-hard的合成结果,而采用Whisper-large V3进行test-en评估内容一致性。我们采用两个说话人验证(SV)模型评估说话人相似度:WavLM微调SV模型和ERes2Net。

### 3.4 日语和韩语基准

我们准备了两个测试集,分别表示为test-ja和test-ko,用于日语和韩语语音合成的评估。test-ja由从CommonVoice数据集提取的1,000个样本组成,用于测量模型在各种指标上的性能,如WER、SS、MOS。

具体而言,我们随机打乱并配对整个CommonVoice JA-test集作为参考语音和目标语音 spoken。考虑到JA-test集语音文本长度的广泛范围,我们从长度范围8到32个字符随机选择1,000对参考-目标语音作为我们的最终测试集。

对于test-ko,我们选择了1,000个WER低于5%且无删除或插入错误的语音样本,使用Whisper-Large V3[54]作为ASR模型。这些样本被用作韩语语音合成的参考语音。对于输入文本,我们从剩余数据中随机选择1,000个文本样本。我们已经发布了这些两个测试集的提示语音、提示转录和输入文本列表,以促进结果再现。通过提供此开源数据,我们旨在建立评估日语和韩语TTS模型的基准。使用Whisper-large V3作为日语和韩语评估的ASR模型。

---

## 4 实验结果

### 4.1 语音标记器评估

理想的语音标记器应该有效利用码本,以高保真度保持信息,并展示说话人独立性。在本节中,我们从四个方面评估我们的监督语音标记器:1)码本利用率;2)整个编码器中的ASR错误率;3)不同说话人的标记可视化;4)说话人识别训练。

表4显示了码本利用率和ASR错误率。结果表明,基于FSQ的标记器充分利用码本,并从ASR角度保持更多有效信息,表明FSQ保持了更多语义信息。

我们进一步通过t-SNE可视化分析FSQ的特性。作为TTS任务的上游模型,标记器应努力最小化说话人身份信息与语音信号的纠缠。我们从VoxCeleb1数据集的每个说话人选择100个语音样本并可视化相应标记。如图4(a)和(b)所示,很明显,量化之前,Encoder1的输出在不同说话人之间展示不同分布。相比之下,量化表示的分布几乎无法区分。此外,图4(c)还显示标记器充分利用了码本。

随后,采用S3prl工具包[55]通过执行说话人识别(SID)任务进一步评估说话人纠缠。我们使用带有FSQ的Sensevoice-large编码器作为上游特征提取器,并在量化前后训练SID任务。图5显示了训练期间的准确度曲线。带量化标记的SID层不收敛,证明标记器在说话人信息上的解耦功能。

**表4: Sensevoice-large编码器中VQ和FSQ的比较。C.V.表示CommonVoice基准。**

| 方法 | 码本大小 | 利用率 | C.V. EN | C.V. CN | Fluers EN | Fluers CN |
|------|----------|--------|---------|---------|-----------|-----------|
| VQ | 4,096 | 963 (23%) | 18.26 | 11.56 | 7.65 | 5.03 |
| FSQ | 6,561 | 6,561 (100%) | 10.67 | 7.29 | 6.58 | 4.43 |

### 4.2 与基线的比较结果

我们首先在有限英语文本域上评估CosyVoice 2模型,并与几个开源模型如ChatTTS[56]、GPT-SoVITs[57]、OpenVoice[58]、ParlerTTS[59]、EmotiVoice[60]及其前一个版本CosyVoice[34]进行比较。客观结果如表5所示,包括内容一致性(WER)、语音质量(NMOS)和说话人相似度(SS)。

从表中可以看出,CosyVoice 2在Librispeech test-clean集上实现了最先进的性能,在所有评估指标上超越了所有基线模型。值得注意的是,CosyVoice 2甚至展示了比人类话语更高的内容一致性、语音质量和说话人相似度,表明其人类同等合成质量。

**表5: 基线和CosyVoice 2在Librispeech test-clean子集上的内容一致性(WER)、说话人相似度(SS)和语音质量(NMOS)结果。使用Whisper-Large V3作为ASR模型,计算WER前排除标点符号。**

| 模型 | WER(%) | NMOS | SS |
|------|---------|------|----|
| Human | 2.66 | 3.84 | 0.697 |
| ChatTTS[56] | 6.84 | 3.89 | - |
| GPT-SoVITs[57] | 5.13 | 3.93 | 0.405 |
| OpenVoice[58] | 3.47 | 3.87 | 0.299 |
| ParlerTTS[59] | 3.16 | 3.86 | - |
| EmotiVoice[60] | 3.14 | 3.93 | - |
| CosyVoice[34] | 2.89 | 3.93 | 0.743 |
| CosyVoice 2 | 2.47 | 3.96 | 0.745 |
| CosyVoice 2-S | 2.45 | 3.90 | 0.751 |

我们还在常用的测试集上评估CosyVoice 2:SEED test-zh、test-en和test-hard,它们包括来自不同域的各种输入文本和参考语音。CosyVoice 2和基线模型的实验结果如表6所示。

在test-zh集上,CosyVoice 2在CER和SS方面超越了所有开源模型,仅略逊于商业模型SEED-TTS。在test-en集上,CosyVoice 2在WER和SS方面分别排名第四和第三。这可能归因于中英文训练数据量之间的不平衡。我们计划在未来工作中探索数据缩放以增强英语内容一致性。在test-hard集上,离线CosyVoice 2模型在所有比较基线中实现了最先进的性能,展示了其在挑战性合成场景中的鲁棒性。

与人类生成语音相比,CosyVoice 2显示了可比的内容一致性和卓越的说话人相似度。考虑到识别错误也可能来自ASR模型,可以合理地得出结论,CosyVoice 2实现了人类同等合成能力。

我们还评估了流式模式,在表5和6中表示为"CosyVoice 2-S"。对于两种评估设置,流式模式的性能在典型测试情况下几乎无损。仅在挑战性案例中内容一致性略有下降,突出了我们统一流式/非流式框架的优势。

我们发现说话人相似度的结果在不同SV模型上不一致。这可能表明了一个新的研究主题,即如何自动评估TTS模型的说话人相似度。由于不同的TTS模型可能使用不同的SV模型提取说话人信息,使用相同的SV模型评估说话人相似度可以更准确地评估说话人信息的利用。因此,我们在后续实验中使用ERes2Net³评估说话人相似度。

**表6: CosyVoice 2和最近的TTS模型在SEED测试集上的结果。†表示闭源模型。对于说话人相似度,括号内的结果由ERes2Net测量,而括号外的结果由基于WavLM的模型测量。**

| 模型 | test-zh CER(%)↓ | test-zh SS↑ | test-en WER(%)↓ | test-en SS↑ | test-hard WER(%)↓ | test-hard SS↑ |
|------|-----------------|--------------|-------------------|--------------|----------------------|-----------------|
| Human | 1.26 | 0.755 (0.775) | 2.14 | 0.734 (0.742) | - | - |
| Vocoder Resyn. | 1.27 | 0.720 | 2.17 | 0.700 | - | - |
| Seed-TTS†[33] | 1.12 | 0.796 | 2.25 | 0.762 | 7.59 | 0.776 |
| FireRedTTS[35] | 1.51 | 0.635 (0.653) | 3.82 | 0.460 (0.526) | 17.45 | 0.621 (0.639) |
| MaskGCT[18] | 2.27 | 0.774 (0.752) | 2.62 | 0.714 (0.730) | 10.27 | 0.748 (0.720) |
| E2 TTS (32 NFE)†[31] | 1.97 | 0.730 | 2.19 | 0.710 | - | - |
| F5-TTS (32 NFE)[32] | 1.56 | 0.741 (0.794) | 1.83 | 0.647 (0.742) | 8.67 | 0.713 (0.762) |
| CosyVoice[34] | 3.63 | 0.723 (0.775) | 4.29 | 0.609 (0.699) | 11.75 | 0.709 (0.755) |
| CosyVoice 2 | 1.45 | 0.748 (0.806) | 2.57 | 0.652 (0.736) | 6.83 | 0.724 (0.776) |
| CosyVoice 2-S | 1.45 | 0.753 (0.812) | 2.38 | 0.654 (0.743) | 8.08 | 0.732 (0.785) |

### 4.3 模块消融研究

我们对文本-语音语言模型进行了模块消融研究,以评估我们的修改的影响,包括LLM初始化、删除说话人嵌入和使用FSQ。表7说明了CosyVoice 2从前身的逐步开发。

通过用预训练LLM替换随机初始化的语言模型,我们在test-zh和test-hard集上分别实现了内容一致性的18.46%和15.40%相对改进。

接下来,我们从文本到语音语言模型中删除了说话人嵌入,这有助于防止上下文学习中的信息泄露和干扰。这一变化在保持说话人相似度的同时显著减少了内容错误,表明内容信息主要由LM建模,而说话人信息主要由流匹配模型恢复。

最后,通过用FSQ替换VQ,我们实现了CosyVoice 2模型,注意到高得多的内容一致性和不变的说话人相似度。通过充分利用码本,FSQ捕获更多内容信息和上下文变化,导致文本和语音标记之间更好的对齐。

此外,我们通过在基于FSQ的语音标记器训练期间结合音调损失进行了比较实验。我们发现这种方法提高了下游TTS任务的性能,如表7最后一行所示。在CosyVoice的未来版本中,我们计划进行更详细的实验和分析。

**表7: 文本-语音语言模型修改的模态分析。**

| 模型 | test-zh CER(%) | test-zh SS | test-en WER(%) | test-en SS | test-hard WER(%) | test-hard SS |
|------|----------------|------------|------------------|------------|---------------------|--------------|
| CosyVoice | 3.63 | 0.775 | 4.29 | 0.699 | 11.75 | 0.755 |
| + LLM init. | 2.96 | 0.808 | 4.57 | 0.730 | 9.94 | 0.789 |
| + Drop Spk Emb. | 2.56 | 0.804 | 3.81 | 0.740 | 9.66 | 0.778 |
| + FSQ (CosyVoice 2) | 1.45 | 0.806 | 2.57 | 0.736 | 6.83 | 0.776 |
| + Pitch Loss | 1.19 | 0.802 | 2.40 | 0.728 | 6.29 | 0.769 |

我们还进行了另一个模态分析以评估流式模块对合成性能的影响。表8显示了内容一致性和说话人相似度的结果。我们发现流式LM对来自test-zh和test-en集的典型案例影响最小,表明我们统一训练框架的有效性。

流式LM的主要影响在来自test-hard集的挑战性案例中观察到,可能是由于流式模式中上下文信息的丢失。有趣的是,流式流匹配模型相比于离线模式产生了略高的说话人相似度。这可能由于流式模式中初始块的提示到生成比率更高,而离线模式中的提示到生成比率可能非常低,有许多填充标记。

流式流匹配模型对内容一致性的负面影响比流式LM小得多,这归因于CosyVoice 2中的语义-声学解耦建模。

**表8: CosyVoice 2中流式模块影响的模态分析。流式模块的分块大小设置为15。**

| 模型 | LM | FM | test-zh CER(%) | test-zh SS | test-en WER(%) | test-en SS | test-hard WER(%) | test-hard SS |
|------|----|----|---------------|-----------|------------------|------------|---------------------|--------------|
| M1 | Offline | Offline | 1.45 | 0.806 | 2.57 | 0.736 | 6.83 | 0.776 |
| M2 | Offline | Stream. | 1.46 | 0.811 | 2.60 | 0.743 | 7.12 | 0.788 |
| M3 | Stream. | Offline | 1.38 | 0.806 | 2.51 | 0.737 | 7.88 | 0.773 |
| M4 | Stream. | Stream. | 1.45 | 0.812 | 2.38 | 0.743 | 8.08 | 0.785 |

### 4.4 日语和韩语基准结果

除了中英文,CosyVoice 2还支持日语和韩语。我们在构建的日语和韩语测试集上评估了内容一致性、说话人相似度和语音质量。如表9所示,CosyVoice 2在韩语上的所有评估指标上表现明显优于日语。这种差异主要是由于日语和中文之间的字符集重叠,导致在日语语境中出现中文发音。

在未来的工作中,我们计划探索增强多语言合成的语言上下文。由于韩语与其他语言没有字符重叠,其语音合成实现了好得多的性能。另一个问题是数据不平衡。我们认为增加训练数据量可以进一步提高日语和韩语的合成性能。

**表9: CosyVoice 2及其流式对应物在日语test-ja和韩语test-ko测试集上的内容一致性(CER)、说话人相似度(SS)和语音质量(NMOS)。**

| 模型 | test-ja CER(%) | test-ja SS | test-ja NMOS | test-ko CER(%) | test-ko SS | test-ko NMOS |
|------|----------------|------------|-------------|----------------|------------|-------------|
| CosyVoice 2 | 18.79 | 0.630 | 3.42 | 7.98 | 0.707 | 3.73 |
| CosyVoice 2-S | 21.41 | 0.629 | 3.35 | 9.06 | 0.714 | 3.60 |

### 4.5 指令生成结果

为了评估指令生成的性能,我们创建了一个包含290个样本的中文测试集。此集合包括表1中显示的29种指令类型,每种有10个不同的输入文本。我们利用来自五个说话人(三个女性和两个男性)的五个音频提示和说话人嵌入作为流匹配模型的条件。我们的测试在离线模式下进行。

我们客观评估内容一致性(CER)、说话人相似度(SS)和语音质量(NMOS)。主观上,我们使用指令平均意见评分(MOS-I)评估指令的准确性和自然度,范围为1到5。每个样本由10个以中文为母语的说话人评估,评分以0.5为增量。评估标准侧重于语音是否遵循所有指定的指令,如情感表达、语速调整、方言使用和角色扮演。

细粒度控制,包括插入笑声、边笑边说、呼吸控制和强调,针对自然度和准确性进行评估。

如表10所示,CosyVoice 2在内容一致性(CER)、说话人相似度(SS)以及指令控制的准确性和自然度(MOS-I)方面表现出卓越性能,同时保持与CosyVoice-Instruct相当的语音质量。

当从CosyVoice 2中删除输入指令时,MOS-I显著下降;然而,在内容一致性(CER)、说话人相似度(SS)和语音质量(NMOS)方面观察到改进。这表明指令可控性很难从内容文本中隐含地浮现。

**表10: CosyVoice-Instruct、CosyVoice 2和无指令输入的CosyVoice 2在内部中文测试集上的内容一致性(CER)、说话人相似度(SS)、语音质量(NMOS)和MOS-I(指令,评估指令的准确性和自然度)的评估结果。使用Paraformer模型作为ASR系统,CER计算中排除标点符号。方言数据不包括在CER计算中,因为Paraformer模型无法识别中文方言语音。**

| 模型 | CER(%) | SS | NMOS | MOS-I |
|------|---------|----|------|-------|
| CosyVoice-Instruct[34] | 1.72 | 0.797 | 3.94 | 3.09 |
| CosyVoice 2 | 1.52 | 0.804 | 3.94 | 4.06 |
| CosyVoice 2 w/o Instruction | 0.97 | 0.817 | 4.02 | 2.28 |

### 4.6 说话人微调模型结果

在微调阶段,我们使用同一说话人的说话人嵌入进行无监督聚类以确保说话人音色的稳定性。我们证明了一个仅有400个音频录音的目标说话人可以合理地实现良好的语音合成性能,在不同说话人之间客观指标仅观察到轻微变化,如图6所示。

我们的实验表明,大多数说话人可以继承零样本TTS模型的稳健上下文理解和感知能力,从而自然地响应输入文本表达各种情绪和情感。

### 4.7 使用强化学习的LM微调

虽然SFT可以在大多数说话人上提高性能,但Spk E的结果仍然不如基础模型,特别是在英语上。因为Spk E具有更复杂的声音和更快的语速。此外,Spk E只有中文录音可用。因此,我们对Spk E应用强化学习以进行进一步改进。

对于DPO,我们通过SFT模型合成1万个样本对以通过ASR和SS奖励改变LM的偏好偏置。我们还使用可微分的ASR奖励优化LM参数。在RL后,我们使用内容一致性(WER)、说话人相似度(SS)和语音质量(NMOS)在Spk E的测试集上评估模型,并进一步在SeedTTS测试集上评估WER以探索模型是否能够维持对域外或跨语言输入文本的鲁棒性。结果如表11所示。

与预训练基础模型相比,SFT模型显示出更高的说话人相似度和语音质量,然而,WER可能比基础模型更差。我们发现基础模型合成的音频总是比SFT和基准更慢,这对ASR系统更友好。

对于目标说话人数据集,偏好偏置和可微分奖励都可以减少WER,对其他两个指标影响很小。但对于SEED测试集,基于DPO的强化仅受益于中文和英文子集,而硬样本将更差。原因可能是硬样本包含许多重复的单词或短语,它们可能在DPO训练期间被视为拒绝样本。然而,可微分ASR奖励不会遭受这个问题,因为它可以通过ASR后验直接优化TTS系统。这意味着可微分ASR奖励在域外情况中具有更好的泛化能力。最后,我们可以将它们结合以进行进一步改进。

**表11: Spk E上强化学习模型的内容一致性(WER)、说话人相似度(SS)和语音质量(NMOS)比较。**

| 模型 | Inhome Target Speaker WER(%) | Inhome Target Speaker NMOS | Inhome Target Speaker SS | SEED tests(%) zh | SEED tests(%) en | SEED tests(%) hard |
|------|-------------------------------|-----------------------------|-------------------------|------------------|-------------------|---------------------|
| Ground Truth | 6.00 | 3.87 | 0.697 | 1.26 | 2.14 | - |
| CosyVoice 2 | 5.34 | 3.91 | 0.721 | 1.45 | 2.57 | 6.83 |
| CosyVoice 2-SFT | 7.15 | 3.96 | 0.795 | 1.50 | 4.26 | 7.90 |
| + L_ASR | 6.79 | 3.96 | 0.795 | 1.29 | 3.53 | 7.30 |
| + L_DPO | 6.83 | 3.96 | 0.792 | 1.43 | 4.02 | 8.31 |
| + L_ASR + L_DPO | 6.64 | 3.97 | 0.796 | 1.25 | 3.17 | 6.66 |

---

## 5 结论

基于CosyVoice的成功,本报告介绍了CosyVoice 2,一个利用大语言模型的改进流式语音合成模型。通过在单个框架中统一流式和非流式合成,CosyVoice 2实现了人类同等自然度、最小响应延迟,以及在流式模式下几乎无损的合成质量。

关键创新包括用于完全码本利用的有限标量量化、结合预训练文本LLM的简化文本到语音语言模型架构,以及为支持多样化合成场景而开发的分块感知因果流匹配模型。此外,指令TTS能力的改进允许通过细粒度控制情感、口音、角色风格和语音突发来实现更多样化和生动的语音生成。

通过系统修改和优化,CosyVoice 2不仅提供卓越的合成质量,还放宽了部署要求,使其适合流式和非流式应用。我们相信CosyVoice 2代表了可扩展、高质量和交互式文本到语音合成的显著进步。

---

## 6 局限性

CosyVoice 2有几个需要解决的问题。首先,它仅支持有限数量的语言。对于具有重叠字符集的语言,合成性能可能下降,这是未来研究面临的开放挑战。其次,CosyVoice 2不能通过文本指令控制声学特征,如音色,这对于角色扮演应用可能是一个有趣的探索领域。此外,CosyVoice在执行唱歌时表现不佳。

---

## 表1: 自然语言指令和细粒度指令示例

### 自然语言指令

**情感**:
- 高兴(Happy), 悲伤(Sad), 惊讶(Surprised), 愤怒(Angry), 恐惧(Fearful), 厌恶(Disgusted), 冷静(Calm), 严肃(Serious)

**语速**:
- 快速(Fast), 非常快速(Very Fast), 慢速(Slow), 非常慢速(Very Slow)

**方言**:
- 粤语, 四川话, 上海话, 郑州话, 长沙话, 天津话

**角色扮演**:
- 神秘(Mysterious), 凶猛(Fierce), 好奇(Curious), 优雅(Elegant), 孤独(Lonely), 机器人(Robot), 小猪佩奇(Peppa), 等

### 细粒度指令

**语音突发**:
- [laughter], [breath], 等

**语音特征**:
- <laughter></laughter>, <strong></strong>

**示例**:
- 你能用高兴的情感说吗？<|endofprompt|>今天真是太开心了,马上要放假了!I'm so happy, Spring Festival is coming!
- Please speaking very fast.<|endofprompt|>Today is a happy day, full of laughter and joy.
- 请问你能模仿粤语的口音吗？<|endofprompt|>多保重,早休息。
- 尝试一下以机器人的角色和我交流。<|endofprompt|>接收知识光波！
- [laughter]有时候,看着小孩子们的天真行为[laughter],我们总会会心一笑。
- She pursued her dreams with <strong>enthusiasm</strong> and <strong>grit</strong>.
