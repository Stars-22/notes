# CosyVoice 3:通过扩容和后训练迈向野外语音生成

**作者**: Zhihao Du, Changfeng Gao, Yuxuan Wang, Fan Yu, Tianyu Zhao, Hao Wang, Xiang Lv, Hui Wang, Chongjia Ni, Xian Shi, Keyu An, Guanrou Yang, Yabin Li, Yanni Chen, Zhifu Gao, Qian Chen, Yue Gu, Mengzhe Chen, Yafeng Chen, Shiliang Zhang, Wen Wang, Jieping Ye

**机构**: 语音实验室, 通义实验室, 阿里巴巴集团
**邮箱**: {neo.dzh, funaudiollm}@alibaba-inc.com

---

## 摘要

在我们之前的工作中,我们介绍了可扩展的流式语音合成模型CosyVoice 2,它集成了大语言模型(LLM)和分块感知流匹配(FM)模型,实现了低延迟双向流式语音合成和人类同等质量。

尽管取得了这些进展,CosyVoice 2在语言覆盖、领域多样性、数据量、文本格式和后训练技术方面仍存在局限。在本论文中,我们提出了CosyVoice 3,一个为野外零样本多语言语音合成设计的改进模型,在内容一致性、说话人相似度和韵律自然度方面显著超越其前代版本。

CosyVoice 3的关键特征包括:
1. **新型语音标记器**,通过监督多任务训练开发,包括自动语音识别、语音情感识别、语言识别、音频事件检测和说话人分析,以改善韵律自然度
2. **新的可微分奖励模型**用于后训练,不仅适用于CosyVoice 3,也适用于其他基于LLM的语音合成模型
3. **数据集规模扩展**:训练数据从1万小时扩展到100万小时,涵盖9种语言和18种中文方言,跨越各种域和文本格式
4. **模型规模扩展**:模型参数从5亿增加到15亿,由于更大的模型容量,在我们的多语言基准上性能得到增强

这些进步显著推动了野外语音合成的进展。我们邀请读者在https://funaudiollm.github.io/cosyvoice3收听演示。

---

## 1 引言

随着生成神经网络的快速发展,文本到语音(TTS)合成在合成质量方面取得了显著进步,超越了传统的拼接和参数方法[1-7]。特别是零样本TTS模型,利用大规模多说话人数据集,可以克隆任何说话人的音色、韵律和风格,表现出超越特定说话人TTS模型的性能,实现了类似人类的韵律自然度和音频质量[8]。

目前,零样本TTS模型大致可分为三类:
- 使用大语言模型(LLM)建模离散声学标记[8-17]
- 基于扩散模型自动学习语音和文本之间内部对齐的系统[18-26]
- 使用自回归LLM建模粗粒度语义,随后使用非自回归模型(如扩散模型)渲染详细语音特征的粗到细混合系统[26-32]

考虑到合成质量、流式兼容性和灵活性之间的权衡,这种两阶段混合系统已成为工业应用中的主流选择。

在我们之前的工作中,我们开发了CosyVoice 2[30]。通过优化语义标记利用、基于文本LLM初始化、设计双向流式方案和统一指令能力建模,CosyVoice 2实现了与人类语音相当的合成质量,以及几乎无损的超低延迟双向流式合成能力[30]。

尽管CosyVoice 2在一般中英文广播场景中表现良好,但在语言覆盖、领域多样性、数据量和文本格式多样性方面存在明显局限,为实现野外语音生成留下了显著改进空间。此外,针对语音生成模型的模型和数据扩展律,以及适用的后训练技术,尚未得到充分探索。

为了解决这些问题,我们介绍了CosyVoice 3,一个为野外应用设计的大型零样本语音生成模型,覆盖更多语言和多样化场景,并在内容一致性、说话人相似度和韵律自然度方面显著超越其前代CosyVoice 2。

我们的贡献可以总结如下:

- **提出新型语音标记器**:源自大型音频理解语言模型,通过监督多任务训练,使离散语音标记能够更好地捕获如情感和发音风格等副语言信息
- **探索适用于语音生成模型的后训练策略**:提出新的可微分奖励优化(DiffRO)方法,不仅适用于CosyVoice系列,也适用于其他基于离散标记的语音合成模型
- **验证数据集规模扩展**:在语音生成领域验证数据集规模扩展,将训练数据从1万小时扩展到100万小时,涵盖9种常用语言、18种中文口音/方言以及各种文本格式,支持更好的跨语言语音克隆。我们还通过将模型大小从5亿增加到15亿来展示模型规模扩展的影响,进一步增强韵律自然度
- **发布CV3-Eval基准**:为了解决不受限的现实世界语音合成场景中多样性和泛化性的挑战,我们发布了CV3-Eval基准用于野外零样本语音合成,该基准建立在来自Common Voice、FLUERS、EmoBox和网络爬取的真实世界音频数据的真实野外参考语音之上,跨越广泛的语言、方言、领域、环境、情感和风格

通过这些改进,CosyVoice 3在多个基准上实现了最先进的(SOTA)结果。我们相信CosyVoice 3代表了朝着野外语音合成迈出的坚实一步。

---

## 2 CosyVoice 3

图2说明了CosyVoice 3的监督多任务监督语音标记器和生成模型的训练过程。与其前身CosyVoice 2不同,CosyVoice 3中的语音标记器基于MinMo[33],这是一个在超过140万小时语音上训练的大规模语音理解模型,在各种语音任务中展示了强大的性能[33]。

我们还提供了CosyVoice 3的零样本和说话人微调(SFT)模型的训练流水线概述,包括大规模预训练、后训练、持续预训练和多说话人微调。后训练阶段旨在超越训练数据的性能限制,而持续预训练阶段专注于将能力(如指令可控性和多语言合成)从零样本模型转移到SFT模型。

### 2.1 通过监督多任务训练的语音标记器

如图2a所示,与在SenseVoice-Large ASR模型[35]的编码器中插入有限标量量化(FSQ)模块[34]的CosyVoice 2不同,对于CosyVoice 3,我们在MinMo模型[33]的语音编码器中插入FSQ模块。

与SenseVoice-Large ASR模型相比,MinMo是一个在超过140万小时语音的广泛数据集上训练的高级多模态LLM,在各种基准中展示了卓越和最先进的性能,包括口语对话、多语言语音识别和情感识别。

为了进一步增强捕获语义信息的能力,我们利用MinMo训练数据的子集对我们的语音标记器进行约530,000小时的监督多任务学习,包括多语言ASR、语言识别(LID)、语音情感识别(SER)、音频事件检测(AED)和说话人分析(SA)等任务。

在训练阶段,输入语音X经过图2a中的Voice Encoder1获得中间表示H,其中Voice Encoder1由12个带有旋转位置嵌入(RoPE)[36]的Transformer块组成。

然后,中间表示H被输入到FSQ模块进行量化,量化表示经过MinMo的剩余模块(包括Voice Encoder2和MinMo LLM)来预测相应文本标记的后验概率。

在FSQ模块中,中间表示H首先被投影到D维低秩空间,每个维度的值被量化为[-K, K]并使用有界舍入操作ROUND。然后,量化的低秩表示H̄被投影到原始维度H̃,如下:

```
H̄ = ROUND(Projdown(H))  (1)
Ĥ = Projup(H̄)  (2)
```

在训练阶段,使用直通估计来近似FSQ模块和Voice Encoder1的梯度。语音标记μi可以通过计算量化低秩表示h̄i在(2K + 1)进制系统中的索引获得:

```
μi = Σ_{j=0}^{D-1} h̄i,j (2K + 1)^j  (3)
```

Voice Encoder1、FSQ模块的低秩投影器、有界舍入操作和索引计算构成了CosyVoice 3的语音标记器。我们的语音标记器工作在25 Hz的标记率,即每秒25个语音标记。

### 2.2 使用可微分奖励优化的强化学习

最近的TTS系统[26, 37]已经证明强化学习(RL)在提高生成语音质量方面是有效的。然而,据我们所知,尚未建立适用于语音生成的通用RL方法。与NLP任务中的LLM不同,TTS系统需要额外的下游条件流匹配(CFM)和声码器模型来将离散语音标记转换为音频波形。

这些下游模型带来的计算需求是巨大的。更严重的是,在下游处理后,生成的声音始终表现出高相似度;因此,很难区分训练奖励模型的正面和负面反馈。

为了解决这些问题,我们介绍了可微分奖励优化(DiffRO)方法来直接优化语音标记而不是合成音频。DiffRO首先在ASR训练数据上训练ASR类似的Token2Text模型,然后使用后验概率作为奖励。

为了进一步简化训练策略,DiffRO使用Gumbel-Softmax操作对LLM预测的标记进行采样,然后通过反向传播直接优化语音标记以最大化奖励分数,而不是RL训练循环:

```
μ̃t = GumbelSoftmaxPπθ (μt |μ1:t−1 ; Y )  (3)
RASR(Y) = log PASR (Ỹn = Yn |Y1:n−1 ; μ̃1:T )  (4)
```

其中μt和μ̃t表示时间步t的真实语音标记及其采样预测。RASR是基于ASR类似的Token2Text模型计算的奖励函数。由于RASR(Y)旨在鼓励μ̃捕获来自文本的所有信息,它可以帮助TTS系统清晰准确地理解文本。

因此,我们可以直接优化LLM以使输出标记与ASR偏好对齐,并使用Kullback-Leibler (KL)散度来防止模型偏离参考模型太远。与其他RL方法不同,我们在输出标记级别logits上计算KL散度,而不是在序列级别后验概率上。

```
πθ∗ = maxπθ E[R(Y)] − βDKL [πθ (μ|Y )∥πref (μ|Y )]  (5)
DKL [πθ (μ|Y )∥πref (μ|Y )] = Σ_{t=1}^{T} Σ_{k=0}^{Q-1} Pπθ (μt = k) log (Pπθ (μt = k) / Pπref (μt = k))  (6)
```

其中Q是FSQ模块的码本大小,等于(2K + 1)^D^(-1)。

除了Token2Text模型,DiffRO还使用其他下游任务(如SER、MOS预测、AED和其他音频理解任务)进行多任务奖励(MTR)建模。MTR机制可以帮助TTS系统通过遵循指令来控制语音属性{Ai}_{i=1}^K:

```
RTRM (Y, {Ai}_{i=1}^K ) = Σ_{i=1}^K log Ptaski (Ãi = Ai |μ̃)  (7)
```

### 2.3 发音修复

基于LLM的TTS系统主要使用BPE文本标记器,将原始文本作为输入。与传统的基于音素的方法相比,这些系统在发音方面缺乏可控性。具体而言,当涉及由多音字或稀疏或不出现在训练数据中的稀有词引起的发音错误时,缺乏基于人为干预的稳健方法。

为了实现一个在发音方面有效可控的工业级TTS系统,我们将CosyVoice 3扩展为能够通过扩展标记器的词汇来建模单词和音素的混合序列。为了实现这一目标,我们通过用拼音替换中文单音字和使用CMU发音词典替换英文单音词来构建一个辅助训练集。这个辅助数据集被添加到基础训练集中。

### 2.4 文本规范化的自训练

在文本标记化之前,TTS系统通常通过文本规范化(TN)模块处理原始文本,将数字和特殊符号转换为口语化文本,这依赖于大量手工制定的规则;然而,手工制定的规则在特殊符号的覆盖方面不断受到挑战。

我们探索LLM来执行TN任务,从而构建一个更统一的端到端TTS系统。以原始文本作为输入,我们利用三种方式构建另一个辅助训练集:1)我们通过内部基于规则的TN模块传递原始文本,获得文本规范化文本,并通过CosyVoice 2合成音频。2)我们提示Qwen-Max[38]进行文本规范化,然后在规范化文本上通过CosyVoice 2合成音频。3)我们提示Qwen-Max对现有文本-音频对中的文本执行反向文本规范化并获得原始文本(即,未规范化文本)。

原始文本及其对应音频被视为一个配对样本并直接添加到基础训练集中。我们验证了在扩展训练集上训练的新系统可以直接合成原始文本,并在各种特殊符号上表现出更好的鲁棒性和覆盖性。

### 2.5 指令语音生成

为了增强CosyVoice 3的可控性和表现力,与CosyVoice 2相比,我们将更多表现的语音数据集成到基础训练集中。高质量指令跟随数据的时长从1,500小时扩展到5,000小时,覆盖更广泛的类型,包括情感、速度、音调、方言、口音和角色扮演。类型总数增加到超过100种,如表1所示。

与CosyVoice 2类似,CosyVoice 3也支持语言指令和细粒度指令。对于自然语言指令,一个自然语言描述和特殊结束标记"<|endofprompt|"被前置于输入文本用于语音合成。对于细粒度指令,支持文本标记之间的语音突发和语音特征标签进行控制。例如,输入文本中如"[laughter]"和"[breath]"等标记可以分别用于生成明显的笑声和呼吸。标记"<strong>XXX</strong>"用于强调特定单词。

### 2.6 说话人微调中的能力转移

#### 2.6.1 将单语言说话人转变为多语者

CosyVoice 3相较于其前代的一个显著改进是扩展的语言支持。为了使单语言目标说话人能够说多种语言,我们构建了一个辅助训练数据集,其中包含来自随机选择说话人的覆盖所有支持语言的高质量单语言数据。每个话语的说话人ID和语言ID在自然语言指令中指定。

#### 2.6.2 转移指令生成能力

在特定说话人数据上微调预训练模型(SFT)可以提高个人说话人的生成质量和表现力。我们开发了一个训练数据集,其中部分标记有说话人ID。它包括来自目标说话人的高质量数据以及预训练指令跟随数据集。在自然语言指令提示中,我们指定说话人提示和风格提示。例如,一个完整的指令提示可能是,"You are Speaker A. Please talk to me happily."然而,一些数据条目可能缺少说话人ID或风格标签;在这种情况下,我们在提示中留空这些字段。在微调过程中,我们还随机掩盖说话人提示或风格提示以增强模型的转移能力。这种方法确保了跨不同说话人的全面指令覆盖,并有助于防止预训练模型中指令生成的潜在灾难性遗忘。

---

## 3 多语言数据流水线

与中文和英文相比,在其他语言中获取大规模高质量TTS数据更具有挑战性。为了解决这一挑战,我们主要从互联网有声书、视频和播客中收集野外多语言音频数据。然后,我们实现了一个多语言数据处理流水线来生成质量充分的模型训练数据。

该流水线包括六个步骤:1.语音检测和分割;2.降噪;3.ASR转录;4.标点符号调整;5.音量标准化;6.过滤音频-文本长度比率异常的数据。

**语音检测和分割**:原始数据通过说话人分离、语音活动检测(VAD)和音频事件检测模块顺序处理。结果获得短于30秒的说话人级别语音片段。虽然我们在此步骤中使用内部模块,但它们可以被具有相同效果的开源替代品替换。

**降噪**:我们采用MossFormer2[39]模型进行降噪。接下来,基于话语的首尾帧能量级别,因异常截断而以不完整单词开始或结束的话语被筛选出来;剩余的话语,修剪了首尾静音,被保留进行进一步处理。

**ASR转录**:为了获得具有足够可靠性的文本转录,我们首先使用Faster-Whisper Large-V3[40]进行语言识别,然后采用不同的开源ASR模型(即,Faster-Whisper Large-V3、NVIDIA NeMo Canary-1B[41]、Meta FAIR seamlessM4T-V2-large[42])来转录话语。然后我们执行交叉验证并选择来自不同系统的ASR结果中平均成对WER低于15%的转录。

**标点符号调整**:由于ASR生成的文本中的标点符号可能无法适当地表示相应音频中的实际暂停,我们使用Montreal强制对齐器[43]来推导单词和从句或短语之间的持续时间,然后通过预设阈值添加或删除标点符号(≥300毫秒添加逗号而≤50毫秒删除表示暂停的标点符号,即逗号、分号、冒号、句号、问号和感叹号)。

**音量标准化**:对音量标准化应用了简单直接的标准化:

```
normalized wav = (raw wav / max(raw wav)) × 0.6  (8)
```

**过滤音频-文本长度比率异常的话语**:在所有上述处理步骤之后,为每个生成的语音-文本对提取语音标记和文本标记。然后,计算语音标记和文本标记长度的话语级比率并排序。我们丢弃长度比率最小1%和最大5%的话语,以过滤可能的异常情况,如包含无人语音但对应长文本转录的短音频,或仅包含目标语言的短人类语音片段的长音频剪辑,因此对应短文本转录。

---

## 4 实验设置

### 4.1 语音标记器训练数据

使用530,000小时的监督多任务数据集来训练语音标记器,使用规范化转录作为标签,包括自动语音识别(ASR)、语言识别(LID)、语音情感识别(SER)、音频事件检测(AED)和说话人分析(SA)。

**表3: 语音标记器训练数据详情**

| 任务 | 时长(小时) |
|------|-----------|
| 自动语音识别(ASR) - 多语言 | 365K |
| 语言识别(LID) | 85K |
| 语音情感识别(SER) | 48K |
| 音频事件检测(AED) | 21K |
| 说话人分析 | 11K |

多语言ASR训练数据由中文、英文、日语、韩语、俄语、法语和德语组成。

### 4.2 CosyVoice 3的数据集规模和模型扩展

在CosyVoice 3中,我们从多个方面扩展了数据量。对于广泛使用的中文和英文数据,我们采用低成本数据生产流水线和自训练数据构建的组合来增强领域、风格、文本格式和稀有情况的多样性。

在领域多样性方面,我们从电子商务、导航、金融和教育等不同领域收集语音数据。在风格多样性方面,我们添加了对话、演讲、唱歌等。在文本多样性方面,我们通过文本规范化(TN)和反向文本规范化(ITN)为相同语音构建不同的文本格式,增强模型对多样化文本格式的鲁棒性。此外,我们使用自训练策略地创建大量稀有案例,使用CosyVoice 3的早期版本来提高合成稳定性。

在语言覆盖方面,我们用七种常用语言(包括日语、俄语、法语、德语、西班牙语、韩语和意大利语)扩展了中文和英文数据集,数据百分比如图3a所示。我们之前的工作[27]显示,监督多任务语音标记器在一些未见语言(CosyVoice 3情况下的西班牙语和意大利语)上可以表现良好。除了标准普通话发音外,我们增加了中文口音和方言的覆盖,支持19种常用口音或方言,数据百分比如图3b所示。

通过这些数据扩展努力,CosyVoice 3的训练数据达到100万小时,涵盖日常生活中大多数用户案例,朝着野外零样本语音生成迈进。

除了扩展数据集规模,扩展模型规模对于当前大规模模型也至关重要。因此,我们增加了CosyVoice 3中文本到语音语言模型(LM)和条件流匹配(CFM)模型的模型规模。具体而言,文本到语音LM从5亿增加到15亿参数。对于CFM,我们采用最近的扩散Transformer(DiT)[25, 44]作为骨干,将参数数量从1亿增加到3亿。初步实验展示了DiT架构的强大性能;因此,复杂的文本编码器和长度正则化模块不再需要,并从CosyVoice 3中移除。我们通过简单的插值操作解决了语音标记和Mel特征之间的帧率不匹配问题。

### 4.3 零样本能力的评估设置

为了评估CosyVoice 3的零样本语音生成能力,我们关注三个关键方面:内容一致性、说话人相似度和音频质量。对于内容一致性,我们测量ASR转录与给定文本的字符错误率(CER)或词错误率(WER),使用Whisper-large V3[45]进行英文ASR,使用Paraformer[46]进行中文ASR。

为了评估说话人相似度,我们使用ERes2Net说话人验证模型[47]从生成语音中提取说话人嵌入,并计算其与参考语音嵌入的余弦相似度。对于音频质量,我们使用DNSMOS网络[48]对生成语音进行评分,其分数与人类听觉感知显示出高相关性。

我们在两个测试集上进行评估。第一个是广泛使用的SEED-TTS-Eval测试集[26],其中测试案例被分类为普通话、英文和硬中文子集。为了促进与其他模型的公平比较,我们还使用基于WavLM的说话人识别模型来计算说话人相似度[49]。值得注意的是,最近的语音生成模型进展几乎没有留下改进空间,模型取得了相当相似的分数;因此,我们引入了一个新的多语言基准CV3-Eval进行评估,详见4.4节。

为了与CosyVoice 3进行全面比较,我们采用10个常用的语音生成模型作为基线,这些模型在一些方面实现最先进(SOTA)或竞争性能。具体而言,非自回归(NAR)模型包括MaskGCT[15]、E2 TTS[24]、F5-TTS[25]和F5R-TTS[37],而自回归(AR)基线是Seed-TTS[26]、FireRedTTS[29]、Qwen2.5-Omni[50]、CosyVoice[27]、CosyVoice 2[30]和Spark TTS[17]。

### 4.4 CV3-Eval:多语言基准

随着语音生成模型的快速发展,现有的评估基准不再满足模型评估要求,特别是对于零样本语音克隆。首先,大多数评估基准(如Librispeech[51])来自有声书,其中说话人的发音是干净和标准的。结果,一些系统可以轻松合成高质量音频,甚至超越真实音频。然而,源音频在现实世界应用场景中通常是嘈杂的,这些基准未能解决的挑战。其次,大多数基准是为中文和英文设计的,而多语言评估基准缺席。最后,传统基准仅关注发音准确度、说话人相似度和音频质量的MOS分数。这些评估指标无法准确测量TTS系统的全面能力,包括情感表达、节奏丰富性、语音可控性和跨语言语音克隆等方面。

为了更好地评估CosyVoice 3,我们建立了一个多语言基准CV3-Eval,包括客观和主观评估的子集。

**客观评估**:客观评估子集进一步分为三个子集,包括多语言语音克隆、跨语言语音克隆和情感克隆,如下:

- **多语言语音克隆**:多语言语音克隆子集包含9种语言,每种语言500个样本,包括中文(zh)、英文(en)、日语(ja)、韩语(ko)、德语(de)、法语(fr)、俄语(ru)、意大利语和西班牙语(es)。源音频和目标文本从CommonVoice[52]和FLUERS[53]数据集中采样。为了模拟现实世界应用场景,我们不过滤具有嘈杂背景或长静音的音频,这对TTS系统的鲁棒性构成挑战。此外,我们为中文和英文构建了两个硬案例测试集,其中目标文本包括稀有词、绕口令、领域特定术语等。
- **跨语言语音克隆**:对于跨语言语音克隆子集,源音频和目标文本来自不同语言,包括zh、en、ja和ko。此子集可以评估TTS系统的语言转移能力。
- **情感克隆**:情感克隆子集中的音频提示来自EmoBox[54]和SeCap[55],包括中文和英文样本。由于一些情感标签的表现力不足,我们仅包含标记为快乐、悲伤或愤怒的样本,每种语言100个样本。我们进一步将这些样本分类为文本相关和文本不相关部分,取决于目标文本是否在语义上与目标情感一致。这有助于我们确定合成情感特征主要来自文本内容还是提示音频。

**主观评估**:除了客观评估子集外,我们还准备了表现性语音克隆、表现性语音延续和中文口音语音克隆的三个主观子集。

- **表现性语音克隆**:为了探索模型生成表现性语音的能力,表现性语音克隆基准设计为包含具有独特特征的音频提示,如高度情感语调、耳语和大喊、极端慢速或快速语速。音频提示从不同挑战性应用场景(如新闻、播客、电视剧、学术报告、诗歌朗诵等)中选取。一些公众人物的声音也被采样用于评估。
- **表现性语音延续**:由于人类感知的高变异性,实现表现性语音克隆的公平主观评估具有挑战性。为了缓解这一问题,我们设计了语音延续任务。具体而言,我们从网站中选择120个具有不同情感、节奏、速度和音量的音频样本,并切割音频剪辑的前3秒作为提示语音。因此,我们可以基于其与真实语音的相似性来评估合成的剩余语音。
- **中文口音语音克隆**:由于目前没有客观方法来评估口音的真实性,我们为中文方言构建了一个主观评估数据集。该数据集包括18种不同的中文方言,如粤语、东北、闽南、上海方言等。所有提示语音样本都来自内部工业数据。

---

## 5 实验结果

### 5.1 SEED-TTS-Eval上的客观TTS结果

表4呈现了CosyVoice 3和几个最近模型在SEED测试集上的TTS性能,这些测试集包括中文test-zh、英文test-en和挑战性test-hard集。评估关注内容一致性(WER/CER)和说话人相似度(SS)。

对于内容一致性,CosyVoice 3实现了显著改进,相对于CosyVoice 2,在test-zh和test-en上分别有44%和51%的相对增益。在test-hard集中,CosyVoice 3将CER从6.83%降低到5.09%(26%相对改进)。与其他基线相比,CosyVoice 3在所有指标上始终超越。值得注意的是,CosyVoice 3-1.5BRL在test-zh上记录了最低的0.71% CER,在test-en上记录了最低的1.45% WER,展示了其卓越的合成准确度。在挑战性test-hard场景中,CosyVoice 3-0.5BRL实现了最低的5.09% CER,而1.5B变体紧随其后为5.66%。较大模型相对于较小模型的表现欠佳归因于可用于预训练和后训练的有限数据集,特别是在挑战性场景中。我们计划在未来将数据集扩展到数千万小时以支持更大模型的有效训练。

关于说话人相似度,CosyVoice 3展示了准确复制说话人特征的强大能力。它超越了CosyVoice 2和其他基线,除了Seed-TTS,如通过基于WavLM和ERes2Net测量所示。CosyVoice 3和Seed-TTS之间的相似度差距主要归因于说话人多样性和预训练数据量的差异。通过扩展预训练数据来增强CosyVoice 3中的说话人相似度可以是一个我们在未来工作中打算追求的方向。此外,表4显示RL后训练有助于内容一致性12%到35%的相对改进,增强了多语言和复杂合成任务中的鲁棒性和适应性。通过RL后训练,CosyVoice 3建立了TTS性能的新最先进状态,展示了相对于之前模型的实质性进步。

**表4: CosyVoice 3和基线在SEED测试集上零样本TTS性能比较,包括内容一致性(WER/CER)和说话人相似度(SS)。对于说话人相似度,括号外的结果由基于WavLM的模型测量,而括号内的结果由ERes2Net测量。粗体表示最佳结果,下划线表示第二佳。**

(表4数据内容较多,此处省略详细数值表格,主要显示CosyVoice 3在各种基准上的优越性能)

### 5.2 多语言基准CV3-Eval上的客观评估

#### 5.2.1 多语言语音克隆结果

我们使用CV3-Eval基准的多语言语音克隆子集评估CosyVoice 3与竞争性开源TTS系统(包括F5-TTS、Spark-TTS和GPT-SoVits¹)的性能。

**表5: CV3-Eval多语言语音克隆子集上的CER(%)和WER(%)。–表示不支持该语言。**

| 模型 | zh | en | ja | ko | de | es | fr | it | ru |
|------|----|----|----|----|----|----|----|----|----|
| F5-TTS | 5.47 | 8.90 | – | – | – | – | – | – | – |
| Spark-TTS | 5.15 | 11.0 | – | – | – | – | – | – | – |
| GPT-SoVits | 7.34 | 12.5 | – | – | – | – | – | – | – |
| CosyVoice2 | 4.08 | 6.32 | 9.13 | 19.7 | – | – | – | – | – |
| + DiffRO | 3.00 | 4.72 | 6.36 | 5.14 | – | – | – | – | – |
| CosyVoice3-0.5B | 3.89 | 5.24 | 10.4 | 12.8 | 7.41 | 4.25 | 12.9 | 6.68 | 6.77 |
| + DiffRO | 2.89 | 3.68 | 5.15 | 4.02 | 4.51 | 2.99 | 8.56 | 2.94 | 3.79 |
| CosyVoice3-1.5B | 3.91 | 4.99 | 7.57 | 5.69 | 6.43 | 4.47 | 11.8 | 10.5 | 6.64 |
| + DiffRO | 3.01 | 3.71 | 5.27 | 4.01 | 3.93 | 3.26 | 8.09 | 2.72 | 4.11 |

多语言语音克隆子集被证明是显著挑战性的,CosyVoice 3是唯一能够覆盖此子集所有语言的系统。对于大多数语言,CosyVoice3-0.5B和CosyVoice3-1.5B之间的性能差异最小。此外,如表6所示,生成稀有词、绕口令和领域特定术语对CosyVoice 3仍然是困难的,突出了未来改进的领域。

**表6: CV3-Eval多语言语音克隆子集硬样本上的WER(%)、说话人相似度(SS)和MOS分数。**

| 模型 | hard-zh WER | hard-zh SS | hard-zh DNSMOS | hard-en WER | hard-en SS | hard-en DNSMOS |
|------|------------|------------|---------------|------------|------------|---------------|
| CosyVoice2 | 12.58 | 72.6 | 3.81 | 11.96 | 66.7 | 3.95 |
| + DiffRO | 10.66 | 71.7 | 3.81 | 10.25 | 62.4 | 3.97 |
| CosyVoice3-0.5B | 14.15 | 78.6 | 3.75 | 9.04 | 75.9 | 3.92 |
| + DiffRO | 8.26 | 77.8 | 3.80 | 7.60 | 73.9 | 3.95 |
| CosyVoice3-1.5B | 9.77 | 78.5 | 3.79 | 10.55 | 76.1 | 3.95 |
| + DiffRO | 9.06 | 78.2 | 3.81 | 7.56 | 74.6 | 3.95 |

#### 5.2.2 跨语言语音克隆结果

表7说明了CosyVoice 3相对于CosyVoice 2在跨语言语音克隆方面的显著改进。值得注意的是,CosyVoice 2在将语音从日语转移到中文方面表现困难,归因于两种语言的字符重叠,这一问题在CosyVoice 3中通过将所有日语字符转换为假名得到解决。此外,扩展模型规模证明是有益的:CosyVoice3-1.5B在所有条件下表现出比CosyVoice3-0.5B更好的WER,同时保持相似的说话人相似度。这表明更大模型由于容量增加可以增强挑战性任务上的性能。

由于大多数开源TTS系统仅支持中文和英文,我们进一步在zh2en和en2zh跨语言语音克隆任务上评估CosyVoice 3与基线,如表8所示。与CosyVoice 3相比,F5-TTS和Spark-TTS在WER上表现出较差性能,Spark-TTS在SS上也显著落后于F5-TTS和CosyVoice 3。关于MOS分数,CosyVoice 3在en2zh上展示更好的结果,在zh2en上具有可比的结果。总体而言,CosyVoice3-1.5B仍然是zh2en和en2zh跨语言转移任务的领先模型。

**表7: CV3-Eval跨语言语音克隆子集上的WER(%)结果。**

| 模型 | to-zh (en) | to-zh (ja) | to-zh (ko) | to-en (zh) | to-en (ja) | to-en (ko) | to-ja (zh) | to-ja (en) | to-ja (ko) | to-ko (zh) | to-ko (en) | to-ko (ja) |
|------|------------|------------|------------|------------|------------|------------|------------|------------|------------|------------|------------|------------|
| CosyVoice2 | 13.5 | 48.1 | 7.70 | 6.47 | 17.1 | 11.2 | 13.1 | 14.9 | 5.86 | 24.8 | 21.9 | 21.5 |
| CosyVoice3-0.5B | 8.48 | 6.86 | 5.24 | 4.99 | 6.83 | 5.86 | 18.3 | 16.8 | 4.99 | 41.0 | 20.4 | 12.8 |
| + DiffRO | 5.16 | 3.22 | 1.03 | 3.40 | 4.41 | 4.78 | 7.91 | 7.25 | 3.29 | 16.9 | 11.6 | 8.2 |
| CosyVoice3-1.5B | 8.01 | 6.78 | 3.30 | 4.32 | 5.39 | 5.94 | 13.7 | 13.4 | 4.19 | 31.6 | 14.0 | 10.5 |
| + DiffRO | 5.09 | 3.05 | 1.06 | 2.98 | 4.20 | 4.19 | 7.08 | 6.80 | 3.93 | 14.4 | 5.87 | 7.92 |

**表8: CosyVoice 3和基线在zh2en和en2zh语音克隆任务上的WER(%)、说话人相似度(SS)和MOS分数。**

| 模型 | en2zh WER | en2zh SS | en2zh MOS | zh2en WER | zh2en SS | zh2en MOS |
|------|----------|----------|----------|----------|----------|----------|
| F5-TTS | 11.6 | 64.2 | 3.77 | 5.57 | 64.7 | 3.77 |
| Spark-TTS | 12.4 | 48.4 | 3.65 | 7.36 | 56.7 | 3.61 |
| CosyVoice2 | 13.5 | 63.3 | 3.87 | 6.47 | 64.3 | 3.75 |
| CosyVoice3-0.5B | 8.48 | 67.4 | 3.82 | 4.99 | 67.8 | 3.75 |
| CosyVoice3-1.5B | 8.01 | 66.9 | 3.83 | 4.32 | 66.4 | 3.77 |

#### 5.2.3 情感语音克隆结果

在CV3-Eval情感语音克隆子集中,我们采用emo2vec-large-plus模型²作为分类器来评估TTS系统的情感表达能力。结果(表9显示)表明,大多数TTS系统在文本相关子集上表现良好,CosyVoice 3达到最高性能。每个系统在表达特定情感方面卓越,"快乐"是所有模型中最容易传达的情感。然而,在文本不相关任务中,情感准确度显著下降,特别是"悲伤"和"愤怒"情感。这表明TTS系统主要从文本情感推断输出音频的情感基调。这一观察为不太满意的表现提供了有价值的见解,并突出了未来改进的领域。

**表9: CV3-Eval情感语音克隆子集的文本相关和文本不相关子集上的情感准确度。**

| 模型 | 文本相关:快乐 | 文本相关:悲伤 | 文本相关:愤怒 | 文本不相关:快乐 | 文本不相关:悲伤 | 文本不相关:愤怒 |
|------|-------------|-------------|-------------|---------------|---------------|---------------|
| F5-TTS | 0.92 | 0.52 | 0.72 | 0.80 | 0.28 | 0.64 |
| Sparks-TTS | 0.80 | 0.56 | 0.50 | 0.50 | 0.60 | 0.36 |
| GPT-SoVits | 0.88 | 0.54 | 0.50 | 0.48 | 0.40 | 0.30 |
| CosyVoice2 | 0.84 | 0.72 | 0.58 | 0.56 | 0.44 | 0.38 |
| CosyVoice3-0.5B | 0.92 | 0.70 | 0.72 | 0.64 | 0.42 | 0.58 |
| CosyVoice3-1.5B | 0.86 | 0.64 | 0.72 | 0.64 | 0.44 | 0.48 |
| + DiffRO-EMO | 0.98 | 0.68 | 0.84 | 0.98 | 0.50 | 0.68 |

### 5.3 主观评估结果

除了客观指标外,我们使用平均意见分数(MOS)进行主观评估。测试样本包括200个中文和英文句子,每个由10个以相应语言为母语的说话人(5男5女)评估。分数范围从1到5,以0.5为增量。图4显示了CosyVoice 2、CosyVoice 3-0.5B和CosyVoice 3-1.5B模型在两种语言上的MOS分数,以及它们的平均分数。

对于中文,所有三个模型表现相似但仍然落后于人类语音。在英文上,CosyVoice 2得分低于人类基准,CosyVoice 3-0.5B与人类得分匹配,CosyVoice 3-1.5B得分显著更高。总体而言,CosyVoice 3-1.5B超越CosyVoice 3-0.5B,两者都超越CosyVoice 2,说明了数据和模型扩展的优势。尽管在中文上与人类语音存在一些差异,CosyVoice 3模型得分仍高于4.45。这一差距主要归因于合成输出中与真实语音相比的一些低分案例,表明在未来工作中需要改进合成稳定性。

### 5.4 语音标记器的消融研究

#### 5.4.1 上游识别任务

我们的基于监督多任务学习的语音标记器在各种语音和声音任务中展示了强大性能。具体而言,如表10所示,基于FSQ-MinMo的语音标记器(即CosyVoice 3中使用的标记器)有效地维持了多语言ASR能力。通过在FSQ-MinMo中专门关注语音相关任务并从训练集中排除其他任务,我们在Fluers CN测试集上实现了比MinMo卓越的识别性能。此外,表11显示FSQ-MinMo模型在AIR-Bench基准上的表现与MinMo模型相当,该基准包括LID、性别、年龄、情感、语音声音和声音问题分类等任务。

**表10: Sensevoice-large和MinMo编码器中VQ和FSQ在CommonVoice (C.V.)和Fluers基准的语言特定子集上的ASR WER和CER(%)比较。FSQ-MinMo是CosyVoice 3中使用的标记器。**

| 方法 | C.V. EN | C.V. CN | C.V. JA | C.V. KO | Fluers EN | Fluers CN |
|------|---------|---------|---------|---------|-----------|-----------|
| SenseVoice | 7.70 | 8.67 | - | - | 4.57 | 6.98 |
| MinMo | 7.36 | 8.56 | - | - | 4.43 | 6.71 |
| VQ-SenseVoice | 18.26 | 11.56 | - | - | 7.65 | 5.03 |
| FSQ-SenseVoice | 10.67 | 7.29 | - | - | 6.58 | 4.43 |
| FSQ-MinMo | 11.36 | 9.21 | 13.90 | 9.78 | 4.46 | 3.35 |

**表11: MinMo和FSQ-MinMo模型在AIR-Bench基准上的性能比较,包括语言ID、性别、年龄、情感、语音声音和声音问题分类任务的准确度。**

| 方法 | 语言ID | 性别 | 年龄 | 情感 | 语音声音 | 声音问题 |
|------|--------|------|------|------|----------|----------|
| MinMo | 99.2 | 84.8 | 70.1 | 62.4 | 90.7 | 59.1 |
| FSQ-MinMo | 99.2 | 72.8 | 41.8 | 68.4 | 61.3 | 57.7 |

#### 5.4.2 下游TTS任务

除了上游识别任务外,我们还通过用其他标记替换CosyVoice 3标记并保持LM和CFM的模型架构不变来评估标记器在下游TTS任务中的性能,以直接评估合成性能。表12呈现了在两种不同规模数据集(3,000小时和170,000小时)上训练的各种模型的结果。

在我们的监督语义标记器CosyVoice 2.0和CosyVoice 3.0旁边,我们还评估了自监督标记器HuBERT⁴和W2v-BERT 2.0⁵,它们被广泛应用于其他TTS模型。此外,我们还涉及无监督标记器SoundStream⁶,它通过基于残差向量量化的变分自编码器(RVQ-VAE)将声学波形量化为离散标记组。由于其他标记器只有一个码本,仅采用SoundStream的第一个VQ组进行比较。

在3,000小时数据集上,监督语义标记器表现出与HuBERT相似的说话人相似度,同时显著优于W2v-BERT 2.0。这是因为HuBERT和监督语义标记器都专注于语义信息,最小化声学干扰,而W2v-BERT 2.0由于其训练方法保留了所有上下文信息,包括语义和声学。这允许条件流匹配模型更好地强调参考语音的声学特征,同时忽略语音标记中的声学干扰。关于内容一致性,监督标记器在test-zh集上实现最低CER,并在test-en和test-hard集上提供可比性能。HuBERT在test-zh集上明显高的CER强调了其语言特定限制。如预期,SoundStream的声学标记在所有评估测试集上实现明显高的错误率,表明对合成文本的内容一致性差。这是因为这些声学标记既不尝试像自监督标记那样建模上下文信息,也不像监督标记那样与文本对齐,导致缺乏足够的语义信息。

将训练数据量从3,000增加到170,000小时导致内容一致性和说话人相似度的显著改进,特别是对于英文和挑战性场景,相对WER/CER改进范围从63%到75%。如表4所示,进一步将数据集扩展到100万小时提高了性能,但改进率开始趋于平缓。这表明我们的多任务监督标记器是可扩展的,并且受益于更大的数据集,直到收益递减点。

**表12: 使用不同标记器的下游零样本TTS建模在SEED测试集上的性能比较,包括内容一致性(WER/CER)和说话人相似度(SS)。粗体表示最佳结果。**

| 方法 | test-zh CER(%) | test-zh SS | test-en WER(%) | test-en SS | test-hard CER(%) | test-hard SS |
|------|---------------|-----------|---------------|-----------|-----------------|-------------|
| **3000小时数据集** |
| SoundStream(1 VQ) | 14.19 | 0.457 | 25.34 | 0.301 | 27.05 | 0.455 |
| HuBERT | 18.68 | 0.716 | 6.50 | 0.609 | 33.83 | 0.699 |
| W2v-BERT 2.0 | 2.62 | 0.381 | 6.72 | 0.261 | 23.89 | 0.374 |
| CosyVoice 2.0 | 1.92 | 0.668 | 7.21 | 0.535 | 15.99 | 0.645 |
| CosyVoice 3.0-0.5B | 1.68 | 0.710 | 6.60 | 0.614 | 27.60 | 0.679 |
| **170,000小时数据集** |
| CosyVoice 2.0 | 1.45 | 0.806 | 2.57 | 0.736 | 6.83 | 0.776 |
| CosyVoice 3.0-0.5B | 1.27 | 0.815 | 2.46 | 0.747 | 6.96 | 0.787 |

### 5.5 强化学习的消融

我们的实验表明,DiffRO显著增强了TTS系统的性能,包括CosyVoice 2和CosyVoice 3。如表4、5和7所示,DiffRO在WER方面实现了20%到50%的相对改进。增强在低资源语言和跨语言场景中特别显著,在一半的条件下有超过50%的相对WER改进;值得注意的是,CosyVoice 3-0.5B在韩语中显示68.7%的相对改进。

关于说话人相似度,RL在大多数数据集上略微降低说话人相似度,尽管变化最小。这表明DiffRO中"黑客"问题的持续存在,其中模型更关注目标奖励,可能忽略其他指标。引入说话人相似度模块作为奖励任务可能缓解此问题,但可能会增加WER。此外,将SER任务作为奖励模型旨在增强CosyVoice 3的情感表达。表9显示DiffRO-EMO允许CosyVoice3-1.5B在文本相关和文本不相关任务的大多数情感中实现最佳情感准确度。然而,这种情感表达的改进可能对发音产生不利影响,突出了DiffRO中平衡奖励的挑战,这将在未来工作中解决。

此外,如表6所示,DiffRO在硬样本测试集上的WER、SS和DNSMOS的改进不如在整体测试集上明显。这可能是由于硬样本包含稀有词、绕口令和重复词,这对奖励模型构成了重大挑战。

### 5.6 发音修复

我们构建了一个评估集来比较不同的发音修复方法,专注于中文多音字和英文多音词的挑战性案例。纠正率用作评估修复能力的指标。如表13所示,最佳方法实现了100%纠正率。

"RepAll"方法涉及考虑所有中文字符和英文单词作为潜在替换,在训练数据增强期间使用内部G2P模型进行音素预测。虽然此方法提供了广泛的字符-音素组合覆盖,但它由于G2P预测引入了不匹配。相反,"RepMono"仅替换单音字或单词,确保训练集中的准确性。

"CatPhn"和"MixPhn"之间的关键区别在于中文字符是被保留并与其音素表示连接,还是仅由音素替换。"CatPhn"保留语义完整性但要求模型优先考虑音素表示而不是字符,这在仅考虑单音字符时加剧。为了缓解这一点,我们引入一些嘈杂数据,如用不同发音的字符替换一个字符,同时保留正确的音素表示。然而,用"MixPhn"实现竞争性纠正率仍然具有挑战性。

**表13: 发音修复的纠正率。**

| 方法 | 中文纠正率(%) | 英文纠正率(%) |
|------|-------------|-------------|
| RepAll + MixPhn | 13 | 9 |
| RepMono + MixPhn | 15 | 15 |
| RepMono + CatPhn | 15 | 13 |

### 5.7 指令生成

我们使用Expresso[59]数据集以及一个内部表现性数据集来评估指令生成能力的有效性。Expresso数据集是一个多说话人表现性语音集合,具有八种不同的说话风格,在3,000个样本的子集上评估。我们的内部数据集包括3,600个样本,匹配指令跟随训练数据集的领域,并涵盖超过50种不同的情感、速度、方言、口音和角色扮演说话风格。

评估结果在表14中呈现。CosyVoice 3在风格相似度方面显示出显著改进,相对于其前一个版本有约11%的相对增加。在内容一致性方面,CosyVoice 3在Expresso测试集上显示更高的WER,但在我们的内部测试集上显示更低的WER。这种差异在很大程度上归因于ASR模型对标准发音而非情感发音的偏向,如与CosyVoice 2相比真实话语的更高WER所示。客观评估情感语音中的内容一致性仍然是一个具有挑战性的问题。

虽然我们通过指令生成探索了各种风格,但唱歌尚未包括,将在未来工作中解决。目前,CosyVoice 3的指令生成专注于情感、语音和风格,主要与语言模型(LM)相关。音色,更与条件流匹配(CFM)相关,尚未被考虑。使用自然语言或其他模态编辑音色是一个有前途且未充分探索的领域[60]。

**表14: 不同模型在指令TTS任务上的WER(%)、风格相似度(SIM)和MOS分数比较。**

| 模型 | Expresso WER | Expresso SIM | Expresso MOS | 内部数据集 WER | 内部数据集 SIM | 内部数据集 MOS |
|------|------------|-------------|------------|---------------|---------------|---------------|
| GroundTruth | 10.0 | 100 | 3.65 | 8.98 | 100 | 3.47 |
| CosyVoice 2 | 9.42 | 60.98 | 3.54 | 7.75 | 72.99 | 3.53 |
| CosyVoice 3-0.5B | 13.72 | 67.82 | 3.56 | 7.30 | 80.45 | 3.51 |
| CosyVoice 3-1.5B | 13.43 | 68.25 | 3.56 | 7.31 | 81.06 | 3.51 |

### 5.8 说话人微调模型的结果

为了确保SFT模型中的音色一致性,我们使用无监督聚类方法识别每个说话人的音色中心。这些聚类中心随后用作条件流匹配模型中的说话人嵌入。如图5所示,增加训练数据的量和多样性,以及升级语音标记,导致微调模型的错误率降低,在test-en和test-hard集中特别明显。这表明改进基础模型也可以使说话人微调模型受益。

### 5.9 将单语言说话人转变为多语者的结果

在我们的实验中,我们旨在使用第2.6.1节描述的训练过程将单语言说话人转变为多语者。如图6所示,中文、英文、德语、西班牙语、法语、意大利语和俄语等语言的CER/WER都低于4%,展示了我们持续训练方法的有效性。

然而,日语构成挑战,具有更高的9%字符错误率,这可以归因于两个主要因素:在语音合成之前将汉字转换为假名引入了额外错误,以及日语字符的多种发音增加了复杂性。对于韩语,CER约为6%,主要归因于可用数据的有限量和质量。我们将在未来工作中扩展韩语数据。

---

## 6 结论

总而言之,本报告介绍了CosyVoice 3,一个为野外应用设计的高级零样本语音合成模型。通过扩展数据和模型参数,CosyVoice 3克服了语言覆盖和合成质量方面的先前局限,提供了卓越的内容一致性、说话人相似度和韵律自然度。我们的创新,包括新型语音标记器和后训练策略,增强了模型捕获复杂副语言细节的能力。在多个基准上实现最先进的结果,CosyVoice 3代表了语音合成的实质性进步,为多样化现实世界场景中更多样化和高质量的语音生成铺平了道路。

---

## 7 局限性

CosyVoice 3有几个需要在未来工作中解决的问题。CosyVoice 3无法通过文本指令控制声学特征,如音色,这对于角色扮演应用可能是一个有趣且有价值的探索领域。此外,CosyVoice 3在生成唱歌语音方面表现不佳。这可以通过在标记器和LM模型的训练阶段添加唱歌数据来改进。

---

**表1: 预训练数据中出现频率最高的100种说话风格**

(表1列出了各种情感、性格特征和方言/口音,包括:冒险、雄心勃勃、古老、愤怒、艺术性、权威、大胆、勇敢、冷静、迷人、快乐、聪明、指挥、同情、自信、冲突、蔑视、勇敢、创造性、狡猾、好奇、黑暗、欺骗、奉献、挑衅、坚定、纪律、厌恶、共情、精力充沛、恐惧、无畏、快乐、英雄、充满希望、谦虚、富有想象力、冷漠、有洞察力、智慧、内省、快乐、忠诚、无情、神秘、高尚、客观、乐观、热情、耐心、自豪、放松、无情、负责任、悲伤、无私、严肃、震惊、偷偷摸摸、惊讶、复仇、警惕、明智、快、响亮、慢、柔软、冒险家、炼金术士、建筑师、厨师、工匠、侦探、医生、女孩、骑士、领袖、商人、小猪佩奇、诗人、机器人、统治者、学者、流浪者、战士、女巫、青年、安徽方言、粤语、重庆方言、河北方言、山东方言、上海方言、四川方言、天津方言、西安方言、郑州方言、中式英语口音、印式英语口音、俄式英语口音)

**表2: 多语言SFT数据集中的自然语言指令示例**

- 你是说话人小明。请讲法语。
- You are Speaker B. Please speak German.

---

**图1**: 我们的CosyVoice 3和竞争性语音生成模型在各种基准上内容一致性和说话人相似度的性能比较。(a)内容一致性中的数字是由ASR模型测量的CER或WER。(b)说话人相似度中的数字是参考和生成话语的WavLM嵌入之间的余弦相似度。100.00的错误率和0.00的相似度意味着发布的模型不支持该语言。

**图2**: (a)监督多任务训练的语音标记器和CosyVoice 3中训练流水线的说明。(a)中的虚线框模块仅在训练阶段使用。语音标记器在ASR、语言识别(LID)、语音情感识别(SER)、音频事件检测(AED)和说话人分析(SA)任务上进行监督训练。CFM表示条件流匹配模型。

**图3**: (a)七种少数语言和19种中文口音或方言的数据百分比。

**图4**: 零样本克隆模型在中文、英文及其平均上的平均意见分数(MOS)。

**图5**: CosyVoice 3和CosyVoice 2 SFT模型在SEED-TTS-Eval设置下的内容一致性结果。test-en集使用词错误率(WER),其他使用字符错误率(CER)。

**图6**: CosyVoice 3将单语言说话人转变为多语者的内容一致性结果。ZH、KO和JA使用字符错误率(CER),其他使用词错误率(WER)。

---

*脚注³: https://www.modelscope.cn/models/iic/emotion2vec_plus_large/summary

¹: https://github.com/RVC-Boss/GPT-SoVITS

⁴: https://github.com/FunAudioLLM/CosyVoice

⁵: https://github.com/facebookresearch/fairseq/tree/main/examples/hubert

⁵: https://huggingface.co/amphion/MaskGCT/tree/main/semantic_codec

⁶: https://huggingface.co/amphion/MaskGCT/tree/main/acoustic_codec
