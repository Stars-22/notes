# CosyVoice：基于监督语义标记的可扩展多语言零样本语音合成器

**作者**: Zhihao Du, Qian Chen, Shiliang Zhang, Kai Hu, Heng Lu, Yexin Yang, Hangrui Hu, Siqi Zheng, Yue Gu, Ziyang Ma, Zhijie Yan

**机构**: 阿里巴巴集团, 中国, 语音实验室
**邮箱**: {neo.dzh,sly.zsl}@alibaba-inc.com

---

## 摘要

近年来,基于大语言模型(LLM)的文本到语音(TTS)技术因其高自然度和零样本能力而逐渐成为主流。在这一范式下,语音信号被离散化为标记序列,由LLM以文本为条件对这些标记序列进行建模。然后使用标记声码器将这些标记化语音重构为原始波形。

显然,语音标记在基于LLM的TTS模型中起着关键作用。当前的语音标记是通过无监督方式学习的,缺乏明确的语义信息和与文本的对齐。在本文中,我们提出使用**监督语义标记**来表示语音,这些标记是通过在多语言语音识别模型的编码器中插入向量量化从多语言语音识别模型中推导出来的。

基于这些标记,我们进一步提出了一个基于编解码器的语音生成合成器**CosyVoice1**,它由一个用于文本到标记生成的LLM和一个用于标记到语音合成的条件流匹配模型组成。

实验结果表明,监督语义标记在内容一致性和零样本语音克隆的说话人相似性方面显著优于现有的无监督标记。此外,我们发现使用大规模数据可以进一步提高合成性能,表明CosyVoice的可扩展性。据我们所知,这是首次在TTS模型中引入监督语音标记。

---

## 1 引言

文本到语音(TTS)技术在近年来取得了显著进步,从机械般的声音发展到几乎与人类说话者无法区分的声音。在这一进步的最前沿是大语言模型(LLMs),它们越来越多地被用于TTS系统中,以生成具有更高自然度的语音并能够以零样本方式合成语音。

这些基于LLM的TTS模型通过将语音信号转换为标记序列,LLM以文本为条件对这些标记序列进行建模来工作。然后使用标记声码器从标记化语音重构原始波形(Kong等人,2020; Défossez等人,2022)。

TTS过程的一个关键方面是语音标记的表示。传统上,标记是通过无监督学习获得的,这可能无法捕获明确的语义信息或与对应文本良好对齐(Hsu等人,2021; Défossez等人,2022)。认识到这一差距,我们的工作引入了从多语言语音识别模型Whisper(Radford等人,2023)中提取的监督语义标记,通过在编码器中集成向量量化。

这一创新实现了更准确的语义表示和与文本的对齐。早期研究表明,带有辅助自动语音识别(ASR)损失的量化器在通用语音模型(USM)上的语音到文本翻译和ASR任务中优于k-means聚类,如Rubenstein等人(2023)所证明。此外,Ye等人(2024)采用Gumbel-Softmax向量量化来提取离散语音表示,这些表示优先考虑ASR相关信息用于ASR任务。然而,这些方法对文本到语音(TTS)的影响仍然不清楚。

此外,利用这些监督标记,我们提出了**CosyVoice**,一个可扩展且高效的零样本TTS合成器。CosyVoice由一个LLM(用于将文本转换为语义标记序列)和一个条件流匹配模型(用于从这些标记合成后续语音)组成。

与之前的系统如TorToise TTS(Betker,2023)不同,后者采用LLM与去噪扩散概率模型(DDPM)(Ho等人,2020)结合,CosyVoice采用条件流匹配方法,因为它已被证明相比传统扩散模型可以加速训练和推理(Le等人,2024)。

虽然现有方法在TTS中引入了流匹配(Le等人,2024; Guo等人,2023; Mehta等人,2023; Guan等人,2023),但它们通常依赖于音素持续时间预测,需要使用补充的音素器和强制对齐器。然而,CosyVoice绕过了这些依赖,提供了一条更直接高效的从文本到语音的路径。

我们的研究以多种新颖的方式对语音生成领域做出了贡献:

- **首次将监督语音标记集成到TTS模型中**,增强了零样本语音克隆中的内容一致性和说话人相似性
- **提出CosyVoice**,一个可扩展的零样本TTS合成系统,它结合了用于文本到标记生成的LLM和用于标记到语音合成的条件流匹配模型,放弃了额外音素器和强制对齐器的需求
- **完善生成语音的质量**,我们将x-vector(Snyder等人,2018)融入LLM中,将语音建模分解为语义、说话人和韵律成分。LLM对语义内容和韵律进行建模,而条件流匹配模型捕获音色和环境信息。我们使用分类器自由引导(Ho和Salimans,2022a)、余弦调度器和掩码条件等技术来优化流匹配过程

我们的实验结果证明了监督语义标记相对于无监督对应物的优越性。此外,CosyVoice的可扩展性通过利用大规模数据提高合成性能得到证明。因此,这项工作代表了朝着自然语音和多功能TTS系统发展的重要一步。

---

## 2 CosyVoice:使用监督语义标记的可扩展TTS模型

如图1(b)所示,我们的CosyVoice由四个组件组成,即**文本编码器**、**语音标记器**、**大语言模型**和**条件流匹配模型**。

具体而言,文本编码器用于对齐文本和语音标记的语义空间,而语音标记器用于提取语义标记,如图1(a)所示。我们采用大语言模型来学习文本编码和语音标记的整个序列,将TTS重新表述为以文本为提示的自回归序列生成问题。

然后,如图1(c)所示,使用条件流匹配模型通过最优路径上的去噪过程将语音标记转换为梅尔谱图。为了获得可感知的信号,使用HifiGAN声码器(Kong等人,2020)以生成的梅尔谱图为输入合成波形。

### 2.1 语音的监督语义标记

在CosyVoice中,采用监督自动语音识别(ASR)模型来推导监督语义语音(S³)标记器的语音。该模型是我们专有SenseVoice ASR模型的微调版本。它在多语言音频数据上训练,具有丰富的音频内容理解能力。

与原始ASR模型不同,我们将编码器分成两部分,并在它们之间插入一个向量量化层。给定梅尔谱图X作为输入,它经过位置编码和Encoder1获得上下文感知表示H:

```
H = Encoder1(PosEnc(X))  (1)
```

然后,引入向量量化器(VQ)来获得离散标记。对于时间步l的隐藏表示hl,码本C中最近嵌入的索引被视为该时间步的语音标记μl:

```
μl = VQ(hl, C) = arg min ||hl - cn||²  (2)
                cn∈C
```

其中||·||²表示L2范数。在训练阶段,码本嵌入通过指数移动平均(EMA)更新:

```
cμl := αcμl + (1 - α)hl  (3)
```

其中α是预定义的衰减系数。语音标记的相应码本嵌入用作量化隐藏表示H̄ = {cμ1, cμ2, . . . , cμL},并通过剩余编码器层Encoder2:

```
H̃ = Encoder2(PosEnc(H̄))  (4)
```

请注意,在剩余编码器层之前,我们添加一个额外的位置编码以增强时间信息。在Encoder2之后,跟随一个基于transformer的ASR解码器,预测文本标签的后验概率:

```
P(Y|X) = ASRDecoder(H̃, Y_{Z-1})  (5)
```

其中Y_{Z-1}表示在教师强制训练方案中左移的文本标签。

### 2.2 用于TTS的大语言模型

在本节中,我们将TTS任务表述为使用大语言模型(LLM)的自回归语音标记生成问题。对于LLM,序列构造是最重要的问题,其构造如下:

```
[S, v, {ȳu}_{u∈[1:U]}, T, {μl}_{l∈[1:L]}, E]  (6)
```

S和E分别表示序列的开始和结束。v是从语音X中使用预训练声纹模型²提取的说话人嵌入向量。

文本编码Ȳ = {ȳu}_{u∈[1:U]}通过使文本通过字节对编码(BPE)标记器和文本编码器获得:

```
Ȳ = TextEncoder(BPE(Y))  (7)
```

由于文本和语音标记处于不同的语义级别,文本编码器用于对齐它们的语义空间并受益于LLM建模。一个开始标识符T被插入在用监督语义标记器如2.1中描述提取的文本编码和语音标记{μl}_{l∈[1:L]}之间。

在训练阶段,我们采用教师强制方案,其中左移序列用作模式输入,原始序列用作期望输出。请注意,在训练期间仅考虑语音标记和E的交叉熵损失:

```
L_LLM = -(1/(L+1)) * Σ_{l=1}^{L+1} log q(μl)  (8)
```

其中μL+1是"序列结束"标记E。q(μl)表示μL的后验概率,它由LLM之后的softmax层预测。

### 2.3 最优传输条件流匹配

在CosyVoice中,采用最优传输条件流匹配(OT-CFM)模型来学习梅尔谱图的分布,并以生成的语音标记为条件从中生成样本。OT-CFM可以比扩散概率模型(DPMs)实现更好的性能,具有更简单的梯度、更容易的训练和更快的生成(Lipman等人,2023; Tong等人,2023; Mehta等人,2023)。

在连续时间归一化流(CNFs)中,从先验分布p₀(X)到梅尔谱图的数据分布q(X)构建概率密度路径。概率密度路径由时间相关向量场νₜ(X) : [0, 1] × ℝ^{L*D} → ℝ^{L*D}定义,它通过以下常微分方程(ODE)生成流φₜ:

```
dφₜ(X)/dt = νₜ(φₜ(X), t)
φ₀(X) ∼ p₀(X) = N(X; 0, I)
φ₁(X) ∼ p₁(X)  (9)
```

其中t ∈ [0, 1]。通过求解初值问题方程(9),我们可以用p₁(X)逼近语音分布q(X)并从中采样。

为了学习向量场νₜ(X),我们定义最优传输(OT)流并强制神经网络通过最小化以下损失来匹配它:

```
L_{OT-CFM} = E_{t,p₀(X₀),q(X₁)}||ωₜ(φ_{OT}ₜ(X₀, X₁)|X₁) - νₜ(φ_{OT}ₜ(X₀, X₁)|θ)||  (10)
```

其中:

```
φ_{OT}ₜ(X₀, X₁) = (1 - (1 - σ)^t)X₀ + tX₁  (11)
ωₜ(φ_{OT}ₜ(X₀, X₁)|X₁) = X₁ - (1 - σ)^tX₀  (12)
```

说话人嵌入v、语音标记{μl}_{1:L}和掩码梅尔谱图X̃₁也被输入到神经网络中以用可学习参数θ匹配向量场:

```
νₜ(φ_{OT}ₜ(X₀, X₁)|θ) = NNθ(φ_{OT}ₜ(X₀, X₁), t; v, {μl}_{1:L}, X̃₁)  (13)
```

X̃₁是通过将从随机起点到结束的连续帧设置为零获得的X₁的掩码版本。

考虑到生成过程在开始时比之后更难,我们为时间步t引入余弦调度器:

```
t := 1 - cos((tπ)/2)  (14)
```

在调度流下,开始时有更多的生成步骤。

分类器自由引导(CFG)已被证明可以改善扩散概率模型的生成质量(Ho和Salimans,2022b; Nichol和Dhariwal,2021; Le等人,2024)。因此,我们提出将CFG适应到条件流匹配模型中。

在训练阶段,我们以固定的0.2概率随机丢弃条件Ψ = {v, {μl}_{1:L}, X̃₁}。通过这种方式,模型可以学习条件和无条件流。在生成期间,向量场修改如下:

```
ν̃ₜ(φ_{OT}ₜ(X₀, X₁)|θ; Ψ) = (1 + β) · νₜ(φ_{OT}ₜ(X₀, X₁)|θ; Ψ) - β · νₜ(φ_{OT}ₜ(X₀, X₁)|θ)  (15)
```

其中β是0.7的引导强度。

### 2.3.1 零样本上下文学习

CosyVoice模型表现出零样本上下文学习能力,允许仅用简短的参考语音样本来复制任意声音。这个过程需要对标记语言模型(LM)的输入序列进行仔细构造,如图2所示。

对于相同语言的提示语音和输入文本,我们将它们合并形成一个统一输入,将提示语音标记视为预生成。使用此输入序列,自回归LM迭代地预测后续标记,直到遇到"序列结束"标记E。

然而,当提示语音和输入文本在语言上不同时,我们省略与提示关联的文本和标记,以防止原始语言的韵律特征影响目标语言。

重要的是要注意,提示文本(对应于提示语音的内容)可以通过人工注释或ASR模型如SenseVoice转录。与提示文本类似,提示标记从提示语音中用S³标记器提取。

生成语音标记后,它们被附加在提示标记之后,为流匹配模型形成复合条件。此外,说话人嵌入和提示语音的梅尔谱图被并入以进一步增强音色和环境一致性。

### 2.4 指令的丰富生成

为了进一步增强CosyVoice的可控性,我们实验整合了额外的指令微调(Ji等人,2023)。CosyVoice-instruct扩展了CosyVoice-base,具有增强的指令跟随能力。

具体而言,它支持对各种方面的可控性,如:
- **说话人身份**(即说话人的特征)
- **说话风格**(包括情感、性别、语速和音高)
- **细粒度副语言特征**(包括插入笑声、呼吸、边笑边说和强调某些单词的能力)

我们使用此训练数据微调CosyVoice-base,而不在自回归语言模型中纳入说话人嵌入。表1显示了说话人身份、说话风格和细粒度副语言特征的一些示例。

---

## 3 数据集

### 3.1 小规模单语言数据集

我们在LibriTTS(Zen等人,2019)语料库上进行实验,该语料库包含来自2,456名英语说话人的585小时。我们遵循官方数据划分,其中"train-clean-100"、"train-clean-360"和"train-other-500"合并用于训练,"dev-clean"用于模型选择。"test-clean"用于构建如(Du等人,2024)中所述的评估集。

### 3.2 大规模多语言数据集

为了训练CosyVoice模型,我们收集了包含多种语言的相当大的数据集。在整个收集过程中,我们利用专用内部工具进行语音检测、信噪比(SNR)估计、说话人分离和分离。

随后,文本标签使用SenseVoice-Large和Paraformer生成。这些标签通过强制对齐(FA)模型进行细化过程,该模型有助于消除低质量数据并增强标点符号的准确性。跨各种语言的训练数据时长的全面细分如表2所示。

表3呈现了不同类型指令的训练数据时长。

---

## 4 实验设置

### 4.1 监督语义语音标记器

对于小规模单语言数据集,我们采用ESPNet Conformer ASR模型作为骨干,并在前六个编码器层后插入向量量化器。有单个4,096个码的码本。前六个编码器层和向量量化器被用作语音标记器。

至于文本标记器,在训练文本上训练sentencepiece模型,词汇量为4,000。我们在Librispeech(Panayotov等人,2015)语料库上从头训练量化增强的ASR模型50个epoch。

对于大规模多语言数据集,我们采用SenseVoice-Large富识别模型(TongyiSpeech,2024)作为骨干。与小规模数据集类似,我们仍然在前六个编码器层后插入向量量化器,使用单个4,096个码的码本。

更多的超参数选择(如量化器插入层和码数)留给未来工作。与单语言实验不同,我们使用预训练检查点来初始化SenseVoice-Large模型而不是从头训练。插入量化器后,我们在八张A800 GPU上进一步微调整个参数210,000个训练步。

### 4.2 CosyVoice模型设置

我们在单语言和多语言实验中训练tiny和正常大小的模型。模型架构设置的细节如表4所示。tiny模型在LibriTTS训练集上用四张V100-32M GPU训练50个epoch,而多语言模型在我们的内部数据集上用64张V100-32M GPU训练800,000步。

Tiny和正常模型的训练学习率分别为10⁻³和10⁻⁴。预热步设置为10,000。

---

## 5 实验结果

### 5.1 S³标记器评估

在表5中,我们展示了向量量化对Librispeech的dev-clean和test-clean集上的识别性能的影响。

从表中可以看出,在ASR编码器中插入向量量化器仅轻微影响识别性能。结果,VQ插入的Conformer ASR模型在"test-clean"和"test-other"集上分别实现可比的3.18%和7.56%的WER。这表明以监督方式训练的标记器可以维持足够的语义信息并与文本对齐。

为了评估多语言S³标记器保留语义信息的能力,我们将量化增强的SenseVoice-L与其原始版本和Whisper-Large V3模型的识别性能进行了比较。模型在Common Voice zh-CN和en基准上使用评估,发现如表6所示。

从表中可以看出,我们的S³标记在中英文测试集上都表现出稳健的识别性能。值得注意的是,在common_voice_zh-CN集上,S³标记超越了Whisper-Large V3模型(TongyiSpeech,2024)的性能,实现了4.14%的相对错误率降低。这表明S³标记与语义内容之间存在显著相关性。

值得注意的是,S³标记器中只有一个码本,字典大小为4,096个条目。

### 5.2 与基线的比较

我们与其他TTS系统在内容一致性和说话人相似性方面比较了我们提出的CosyVoice模型。对于内容一致性,采用ASR模型识别生成的语音。我们报告词错误率(WER)以及插入、删除和替换错误的数量。对于说话人相似性,我们采用ERes2Net模型(Chen等人,2023)提取提示和生成语音的说话人嵌入,它们的原始余弦相似度被视为说话人相似度。实验结果如表7所示。

与其他TTS模型相比,提出的CosyVoice框架即使使用相同的文本和语音标记器也能实现可比的内容一致性和更高的说话人相似度。

比较Exp-1、Exp-2和Exp-3,我们可以看到文本语音标记器对内容一致性至关重要,对说话人相似性微不足道。在Exp 4实验中,我们将单语言文本和语音标记器替换为多语言标记器。仅使用LibriTTS语料库训练模型会降低内容一致性和说话人相似度。

通过参与内部大规模数据集,性能显著提高,达到人类同等质量。

### 5.3 CosyVoice生成质量评估

我们通过检查内容一致性和说话人相似性来评估CosyVoice的语音合成质量。LibriTTS(Zen等人,2019)的"test-clean"子集和AISHELL-3(Shi等人,2021)的测试集分别用于构建英语和中文的评估集。

对于这些集合中的每个文本,我们随机选择一个提示语音。内容一致性使用Whisper-Large V3(Radford等人,2023)进行英语识别和Paraformer(Gao等人,2022)进行中文识别来评估。说话人相似度通过计算生成和提示语音的说话人嵌入的余弦相似度来量化,嵌入使用ERes2Net(Chen等人,2023)提取。

与其他自回归语言模型类似,我们对标记LM采用随机采样解码策略,并使用五个不同的随机种子值0、7、42、123和1,337评估合成过程。生成的评估指标被平均以确定均值和标准差。此外,我们进行了ASR重新排序以展示离线模式中的潜在性能改进。

表8和9分别呈现了英语和中文的结果。在英语数据集上,CosyVoice达到类似人类的内容识别和更高的说话人相似度的人类水平性能。ASR重新排序显著增强了内容一致性,导致词错误率(WER)降低到1.51%。CosyVoice在WER和插入/删除错误数量上超越了ChatTTS,表明卓越的内容一致性。我们未评估ChatTTS的说话人相似度,因为它未发布语音克隆能力。

至于中文结果,CosyVoice生成的语音实现与原始语音相当的CER以及插入和删除错误。ChatTTS在中文方面比英语有更好的生成能力。虽然ChatTTS和CosyVoice达到类似的CER,但ChatTTS产生更多的插入和删除错误。这是由于说话人泄露问题,其中另一个说话人的语气词被意外生成。相反,CosyVoice没有这个问题,插入和删除错误要少得多。

通过ASR重新排序,CosyVoice达到了显著低的1.84% CER。与英语类似,CosyVoice也比原始语音表现出更高的说话人相似度,展示了其有效的语音克隆熟练度。

### 5.4 CosyVoice的情感可控性

为了验证情感可控性,我们使用公共语音情感识别模型emo2vec³。我们为六种情感中的每一种生成和评估了100个英语语音:快乐、愤怒、悲伤、惊讶、恐惧和厌恶。合成文本的内容设计为匹配目标情感。然后我们测量从合成语音中为每种情感预测情感的准确度。

表10显示了CosyVoice-base和CosyVoice-instruct之间情感控制准确度的比较。对于CosyVoice-instruct,输入包括内容文本和说话风格指令(例如,"Happy.<endofprompt>Content Text")。相反,CosyVoice-base仅接收内容文本作为输入。

结果表明,带有情感指令的CosyVoice-instruct相对于CosyVoice-base和没有情感指令的CosyVoice-instruct显示出显著改进。

### 5.5 CosyVoice作为数据生成器

CosyVoice的一个直接应用是作为数据生成器来增强其他任务的训练数据,如ASR、语音到语音翻译(S2ST)。以ASR任务为例,我们在Librispeech语料库上进行实验以评估CosyVoice生成高质量数据的能力。

实验结果如表11所示,其中"Librispeech"表示原始960小时数据。"Syn on LS text"和"Syn on LS text"分别表示来自Librispeech和MLS训练集的文本生成数据。

从表中可以看出,仅在合成数据上训练,ASR模型可以实现与原始Librispeech训练集相当的结果。将它们整合后,观察到识别准确性的显著增强。

一个有趣的发现是,在MLS文本上参与合成数据显著提高了识别性能。这可能表明文本多样性比语音本身的持续时间对ASR任务更关键。这种改进归因于CosyVoice合成样本引入的多样化语言内容。我们评估的结果强调了CosyVoice生成的样本的高质量。

---

## 6 结论

在本文中,我们介绍了CosyVoice,一个可扩展的多语言语音生成模型,它支持零样本上下文学习、跨语言语音克隆、指令生成和情感、副语言特征的细粒度控制。

实验结果表明,CosyVoice的系统架构对于说话人相似性很重要,而文本和语音标记器很大程度上影响内容一致性。此外,我们发现扩大模型规模和数据量可以显著提高性能。

因此,CosyVoice达到人类同等生成质量。

---

## 表1: 说话人身份、说话风格和细粒度副语言特征示例

### 说话人身份
1. Selene'Moonshade',是一个神秘、优雅的舞者,与夜晚有联系。她的动作既迷人又致命。<endofprompt>希望是一件好事。
2. Theo'Crimson',是一个热情、激情的反叛领袖。充满激情地为正义而战,但与冲动作斗争。<endofprompt>你不知道真正的损失。

### 说话风格
1. 一个快乐的女孩,音调高,语速快。<endofprompt>今天阳光明媚。
2. 一个悲伤的女人,音调正常,语速慢。<endofprompt>我重要的考试失败了。

### 细粒度副语言
1. 嗯,那有点可怕[笑声]。
2. 我想我不吃得太多耶[呼吸],而且我确实经常锻炼。
3. 嗯,这几乎涵盖了<笑声>主题</笑声>,嗯感谢你打电话给我。
4. 团队的<strong>团结</strong>和<strong>韧性</strong>帮助他们赢得了冠军。

---

## 表2: 大规模实验中CosyVoice训练数据跨语言的时长(小时)

| 语言 | 时长(小时) |
|------|-----------|
| ZH   | 130,000   |
| EN   | 30,000    |
| Yue  | 5,000     |
| JP   | 4,600     |
| KO   | 2,200     |

---

## 表3: 按类型划分的指令训练数据时长统计

| 类型 | 时长(小时) |
|------|-----------|
| 说话人身份 | 101 |
| 说话风格 | 407 |
| 细粒度副语言 | 48 |

---

## 表5: 在LibriTTS test-clean集上CosyVoice与其他TTS模型的比较,包括内容一致性(WER)和说话人相似度(SS)。非自回归ASR模型Paraformer-en用于快速评估。

| 模型 | 文本标记 | 语音标记 | WER(%) | #INS+DEL | #SUB | SS |
|------|---------|----------|---------|-----------|-------|----|
| Original | - | - | 3.01 | 66 | 200 | 69.67 |
| VALL-E (Wang等人,2023) | Phone | Encodec | 18.70 | 342 | 1312 | 53.19 |
| UniAudio (Yang等人,2023) | Phone | Encodec | 8.74 | 254 | 519 | 47.56 |
| SpearTTS (Kharitonov等人,2023) | Phone | Hubert | 6.14 | 133 | 410 | 51.71 |
| Exp-1-LibriTTS | Phone | Hubert | 7.41 | 325 | 409 | 67.85 |
| Exp-2-LibriTTS | Phone | Sen³ | 5.05 | 122 | 325 | 67.85 |
| Exp-3-LibriTTS | BPEen | Sen³ | 3.93 | 108 | 239 | 67.85 |
| Exp-4-LibriTTS | BPE | S3 | 4.76 | 134 | 287 | 65.94 |
| Exp-4-Large-scale | BPE | S3 | 3.17 | 96 | 184 | 69.49 |

---

## 表10: CosyVoice-base-300M和CosyVoice-instruct-300M之间情感控制准确度比较。"±"连接每个评估指标的均值和标准差。

| 模型 | Happy | Sad | Angry | Surprised | Fearful | Disgusted |
|------|-------|-----|-------|-----------|---------|-----------|
| CosyVoice-base | 1.00±0.00 | 0.45±0.05 | 0.59±0.03 | 0.26±0.02 | 0.88±0.01 | 0.46±0.06 |
| CosyVoice-instruct | 1.00±0.00 | 0.98±0.02 | 0.83±0.04 | 0.64±0.03 | 0.87±0.03 | 0.93±0.02 |
| w/o instruction | 0.98±0.01 | 0.77±0.04 | 0.49±0.12 | 0.28±0.06 | 0.83±0.04 | 0.45±0.16 |

---

## 表11: 通过将CosyVoice视为数据生成器来评估CosyVoice生成质量。人类语音测试集上的词错误率(%)用作评估指标。

| 训练数据 | dev_clean | dev_other | test_clean | test_other |
|---------|-----------|-----------|-----------|-----------|
| Librispeech | 2.77 | 5.84 | 2.79 | 5.97 |
| Syn on LS text | 2.79 | 6.37 | 3.00 | 6.59 |
| Librispeech + Syn on LS text | 2.44 | 5.52 | 2.56 | 5.68 |
| Librispeech + Syn on LS text ×2 | 2.51 | 5.23 | 2.68 | 5.26 |
| Librispeech + Syn on LS, MLS text | 1.93 | 4.43 | 2.04 | 4.53 |
